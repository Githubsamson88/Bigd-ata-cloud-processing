{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiqsRW0av3S4"
      },
      "source": [
        "**# Réalisez un traitement dans un environnement Big Data sur le Cloud**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtBUy8hJwCDO"
      },
      "source": [
        "- Introduction :\n",
        "\n",
        "Dans le contexte actuel de la révolution numérique, la gestion efficace du Big Data est devenue cruciale pour de nombreuses entreprises, notamment dans des secteurs comme l'AgriTech. Dans ce projet, nous allons nous plonger dans le monde du traitement de données massives en utilisant les technologies du Cloud. À travers une collaboration avec la start-up \"Fruits!\", nous avons pour mission de mettre en place une architecture Big Data sur le Cloud AWS afin de traiter un jeu de données d'images de fruits et de leurs labels associés. Ce projet nous permettra de nous familiariser avec les outils et technologies essentiels pour gérer et traiter des volumes massifs de données dans un environnement distribué."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EmJuv36PwHc2"
      },
      "source": [
        "- Plan :\n",
        "\n",
        "I. Contexte et objectifs du projet\n",
        "\n",
        "II. Analyse du travail existant et identification des besoins\n",
        "\n",
        "III. Mise en place de l'architecture Big Data sur AWS\n",
        "\n",
        "IV. Test, démonstration et retour critique\n",
        "\n",
        "V. Conclusion et perspectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k0qA-ga-wOvv"
      },
      "source": [
        "# Déployez un modèle dans le cloud\n",
        "\n",
        "\n",
        "# Sommaire :\n",
        "\n",
        "**1. Préambule**<br />\n",
        "&emsp;1.1 Problématique<br />\n",
        "&emsp;1.2 Objectifs dans ce projet<br />\n",
        "&emsp;1.3 Déroulement des étapes du projet<br />\n",
        "**2. Choix techniques généraux retenus**<br />\n",
        "&emsp;2.1 Calcul distribué<br />\n",
        "&emsp;2.2 Transfert Learning<br />\n",
        "**3. Déploiement de la solution en local**<br />\n",
        "&emsp;3.1 Environnement de travail<br />\n",
        "&emsp;3.2 Installation de Spark<br />\n",
        "&emsp;3.3 Installation des packages<br />\n",
        "&emsp;3.4 Import des librairies<br />\n",
        "&emsp;3.5 Définition des PATH pour charger les images et enregistrer les résultats<br />\n",
        "&emsp;3.6 Création de la SparkSession<br />\n",
        "&emsp;3.7 Traitement des données<br />\n",
        "&emsp;&emsp;3.7.1 Chargement des données<br />\n",
        "&emsp;&emsp;3.7.2 Préparation du modèle<br />\n",
        "&emsp;&emsp;3.7.3 Définition du processus de chargement des images et application <br />\n",
        "&emsp;&emsp;&emsp;&emsp;&emsp;de leur featurisation à travers l'utilisation de pandas UDF<br />\n",
        "&emsp;&emsp;3.7.4 Exécution des actions d'extractions de features<br />\n",
        "&emsp;3.8 Chargement des données enregistrées et validation du résultat<br />\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_yy4zsRGwTxP"
      },
      "source": [
        "# 1. Préambule\n",
        "\n",
        "## 1.1 Problématique\n",
        "\n",
        "La très jeune start-up de l'AgriTech, nommée \"**Fruits**!\", <br />\n",
        "cherche à proposer des solutions innovantes pour la récolte des fruits.\n",
        "\n",
        "La volonté de l’entreprise est de préserver la biodiversité des fruits <br />\n",
        "en permettant des traitements spécifiques pour chaque espèce de fruits <br />\n",
        "en développant des robots cueilleurs intelligents.\n",
        "\n",
        "La start-up souhaite dans un premier temps se faire connaître en mettant <br />\n",
        "à disposition du grand public une application mobile qui permettrait aux <br />\n",
        "utilisateurs de prendre en photo un fruit et d'obtenir des informations sur ce fruit.\n",
        "\n",
        "Pour la start-up, cette application permettrait de sensibiliser le grand public <br />\n",
        "à la biodiversité des fruits et de mettre en place une première version du moteur <br />\n",
        "de classification des images de fruits.\n",
        "\n",
        "De plus, le développement de l’application mobile permettra de construire <br />\n",
        "une première version de l'architecture **Big Data** nécessaire.\n",
        "\n",
        "## 1.2 Objectifs dans ce projet\n",
        "\n",
        "1. Développer une première chaîne de traitement des données qui <br />\n",
        "   comprendra le **preprocessing** et une étape de **réduction de dimension**.\n",
        "2. Tenir compte du fait que <u>le volume de données va augmenter <br />\n",
        "   très rapidement</u> après la livraison de ce projet, ce qui implique de:\n",
        " - Déployer le traitement des données dans un environnement **Big Data**\n",
        " - Développer les scripts en **pyspark** pour effectuer du **calcul distribué**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiW7g9pOwVCx"
      },
      "source": [
        "## 1.3 Déroulement des étapes du projet\n",
        "\n",
        "Le projet va être réalisé en 2 temps, dans deux environnements différents. <br />\n",
        "Nous allons dans un premier temps développer et exécuter notre code en local, <br />\n",
        "en travaillant sur un nombre limité d'images à traiter.\n",
        "\n",
        "Une fois les choix techniques validés, nous déploierons notre solution <br />\n",
        "dans un environnement Big Data en mode distribué.\n",
        "\n",
        "<u>Pour cette raison, ce projet sera divisé en 3 parties</u>:\n",
        "1. Liste des choix techniques généraux retenus\n",
        "2. Déploiement de la solution en local\n",
        "3. Déploiement de la solution dans le cloud"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izc5EgBxweWB"
      },
      "source": [
        "# 2. Choix techniques généraux retenus\n",
        "## 2.1 Calcul distribué\n",
        "\n",
        "L’énoncé du projet nous impose de développer des scripts en **pyspark** <br />\n",
        "afin de <u>prendre en compte l’augmentation très rapide du volume <br />\n",
        "de donné après la livraison du projet</u>.\n",
        "\n",
        "Pour comprendre rapidement et simplement ce qu’est **pyspark** <br />\n",
        "et son principe de fonctionnement, nous vous conseillons de lire <br />\n",
        "cet article : [PySpark : Tout savoir sur la librairie Python](https://datascientest.com/pyspark)\n",
        "\n",
        "<u>Le début de l’article nous dit ceci </u>:<br />\n",
        "« *Lorsque l’on parle de traitement de bases de données sur python, <br />\n",
        "on pense immédiatement à la librairie pandas. Cependant, lorsqu’on a <br />\n",
        "affaire à des bases de données trop massives, les calculs deviennent trop lents.<br />\n",
        "Heureusement, il existe une autre librairie python, assez proche <br />\n",
        "de pandas, qui permet de traiter des très grandes quantités de données : PySpark.<br />\n",
        "Apache Spark est un framework open-source développé par l’AMPLab <br />\n",
        "de UC Berkeley permettant de traiter des bases de données massives <br />\n",
        "en utilisant le calcul distribué, technique qui consiste à exploiter <br />\n",
        "plusieurs unités de calcul réparties en clusters au profit d’un seul <br />\n",
        "projet afin de diviser le temps d’exécution d’une requête.<br />\n",
        "Spark a été développé en Scala et est au meilleur de ses capacités <br />\n",
        "dans son langage natif. Cependant, la librairie PySpark propose de <br />\n",
        "l’utiliser avec le langage Python, en gardant des performances <br />\n",
        "similaires à des implémentations en Scala.<br />\n",
        "Pyspark est donc une bonne alternative à la librairie pandas lorsqu’on <br />\n",
        "cherche à traiter des jeux de données trop volumineux qui entraînent <br />\n",
        "des calculs trop chronophages.* »\n",
        "\n",
        "Comme nous le constatons, **pySpark** est un moyen de communiquer <br />\n",
        "avec **Spark** via le langage **Python**.<br />\n",
        "**Spark**, quant à lui, est un outil qui permet de gérer et de coordonner <br />\n",
        "l'exécution de tâches sur des données à travers un groupe d'ordinateurs. <br />\n",
        "<u>Spark (ou Apache Spark) est un framework open source de calcul distribué <br />\n",
        "in-memory pour le traitement et l'analyse de données massives</u>.\n",
        "\n",
        "Un autre [article très intéressant et beaucoup plus complet pour <br />\n",
        "comprendre le **fonctionnement de Spark**](https://www.veonum.com/apache-spark-pour-les-nuls/), ainsi que le rôle <br />\n",
        "des **Spark Session** que nous utiliserons dans ce projet.\n",
        "\n",
        "<u>Voici également un extrait</u>:\n",
        "\n",
        "*Les applications Spark se composent d’un pilote (« driver process ») <br />\n",
        "et de plusieurs exécuteurs (« executor processes »). Il peut être configuré <br />\n",
        "pour être lui-même l’exécuteur (local mode) ou en utiliser autant que <br />\n",
        "nécessaire pour traiter l’application, Spark prenant en charge la mise <br />\n",
        "à l’échelle automatique par une configuration d’un nombre minimum <br />\n",
        "et maximum d’exécuteurs.*\n",
        "\n",
        "![Schéma de Spark](https://github.com/SatadruMukherjee/Data-Preprocessing-Models/blob/main/img/spark-schema.png?raw=1)\n",
        "\n",
        "*Le driver (parfois appelé « Spark Session ») distribue et planifie <br />\n",
        "les tâches entre les différents exécuteurs qui les exécutent et permettent <br />\n",
        "un traitement réparti. Il est le responsable de l’exécution du code <br />\n",
        "sur les différentes machines.\n",
        "\n",
        "Chaque exécuteur est un processus Java Virtual Machine (JVM) distinct <br />\n",
        "dont il est possible de configurer le nombre de CPU et la quantité de <br />\n",
        "mémoire qui lui est alloué. <br />\n",
        "Une seule tâche peut traiter un fractionnement de données à la fois.*\n",
        "\n",
        "Dans les deux environnements (Local et Cloud) nous utiliserons donc **Spark** <br />\n",
        "et nous l’exploiterons à travers des scripts python grâce à **PySpark**.\n",
        "\n",
        "Dans la <u>version locale</u> de notre script nous **simulerons <br />\n",
        "le calcul distribué** afin de valider que notre solution fonctionne.<br />\n",
        "Dans la <u>version cloud</u> nous **réaliserons les opérations sur un cluster de machine**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QkvvDdcwfpA"
      },
      "source": [
        "## 2.2 Transfert Learning\n",
        "\n",
        "L'énoncé du projet nous demande également de <br />\n",
        "réaliser une première chaîne de traitement <br />\n",
        "des données qui comprendra le preprocessing et <br />\n",
        "une étape de réduction de dimension.\n",
        "\n",
        "Il est également précisé qu'il n'est pas nécessaire <br />\n",
        "d'entraîner un modèle pour le moment.\n",
        "\n",
        "Nous décidons de partir sur une solution de **transfert learning**.\n",
        "\n",
        "Simplement, le **transfert learning** consiste <br />\n",
        "à utiliser la connaissance déjà acquise <br />\n",
        "par un modèle entraîné (ici **MobileNetV2**) pour <br />\n",
        "l'adapter à notre problématique.\n",
        "\n",
        "Nous allons fournir au modèle nos images, et nous allons <br />\n",
        "<u>récupérer l'avant dernière couche</u> du modèle.<br />\n",
        "En effet la dernière couche de modèle est une couche softmax <br />\n",
        "qui permet la classification des images ce que nous ne <br />\n",
        "souhaitons pas dans ce projet.\n",
        "\n",
        "L'avant dernière couche correspond à un **vecteur <br />\n",
        "réduit** de dimension (1,1,1280).\n",
        "\n",
        "Cela permettra de réaliser une première version du moteur <br />\n",
        "pour la classification des images des fruits.\n",
        "\n",
        "**MobileNetV2** a été retenu pour sa <u>rapidité d'exécution</u>, <br />\n",
        "particulièrement adaptée pour le traitement d'un gros volume <br />\n",
        "de données ainsi que la <u>faible dimensionnalité du vecteur <br />\n",
        "de caractéristique en sortie</u> (1,1,1280)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlpaPbIowmb_"
      },
      "source": [
        "# 3. Déploiement de la solution en local\n",
        "\n",
        "\n",
        "## 3.1 Environnement de travail\n",
        "\n",
        "Pour des raisons de simplicité, nous développons dans un environnement <br />\n",
        "Linux Unbuntu (exécuté depuis une machine Windows dans une machine virtuelle)\n",
        "* Pour installer une machine virtuelle :  https://www.malekal.com/meilleurs-logiciels-de-machine-virtuelle-gratuits-ou-payants/\n",
        "\n",
        "## 3.2 Installation de Spark\n",
        "\n",
        "[La première étape consiste à installer Spark ](https://computingforgeeks.com/how-to-install-apache-spark-on-ubuntu-debian/)\n",
        "\n",
        "## 3.3 Installation des packages\n",
        "\n",
        "<u>On installe ensuite à l'aide de la commande **pip** <br />\n",
        "les packages qui nous seront nécessaires</u> :"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HycGGk6OwrzA"
      },
      "source": [
        "## 3.4 Import des librairies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2RiTGILLmnzA",
        "outputId": "cf756063-f387-4b82-d482-72a0bb20a279"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.10/dist-packages (1.34.122)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: botocore<1.35.0,>=1.34.122 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.34.122)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from boto3) (1.0.1)\n",
            "Requirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from boto3) (0.10.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.122->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.122->boto3) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.122->boto3) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspark boto3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "iMVEYRCS3Nzx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from PIL import Image\n",
        "import io\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, pandas_udf, PandasUDFType, element_at, split, udf\n",
        "from pyspark.ml.feature import PCA, StandardScaler\n",
        "from pyspark.ml.linalg import VectorUDT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1zDIe_-W2a-E"
      },
      "source": [
        "- Mettre à jour les packages et installer Java 8:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Ex9jjhi1msx",
        "outputId": "e4a317c8-d4d4-456d-bc50-cc4eff333a01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [1 InRelease 3,626 B/3\r0% [Waiting for headers] [Connecting to security.ubuntu.com (185.125.190.36)] [Connecting to ppa.lau\r                                                                                                    \rHit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ Packages [53.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [127 kB]\n",
            "Get:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [917 kB]\n",
            "Hit:9 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,389 kB]\n",
            "Get:11 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease [18.1 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,160 kB]\n",
            "Get:13 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease [24.3 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/multiverse amd64 Packages [51.5 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-backports/universe amd64 Packages [31.8 kB]\n",
            "Hit:16 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,439 kB]\n",
            "Get:18 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 Packages [27.8 kB]\n",
            "Get:19 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy/main amd64 Packages [47.9 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,088 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,889 kB]\n",
            "Fetched 10.5 MB in 3s (3,858 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4536ccF2iwQ"
      },
      "source": [
        "- Télécharger et installer Hadoop 3.2.1:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "NwuBVXnJkbHh"
      },
      "outputs": [],
      "source": [
        "!wget -q https://archive.apache.org/dist/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz -O hadoop-3.2.1.tar.gz\n",
        "!tar -xzf hadoop-3.2.1.tar.gz > /dev/null\n",
        "!mv hadoop-3.2.1 /usr/local/hadoop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTYaphLa2n1g"
      },
      "source": [
        "- Télécharger et installer Spark 3.4.3:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HqyD5qZA2pTJ",
        "outputId": "b7c1373f-3a7b-44a2-c2f3-c5ff81704f9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hit:1 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:2 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.3/spark-3.4.3-bin-hadoop3.tgz -O spark-3.4.3-bin-hadoop3.tgz\n",
        "!tar -xzf spark-3.4.3-bin-hadoop3.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import findspark\n",
        "findspark.init()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "p_qtyTL91wGY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "os.environ['HADOOP_HOME'] = '/usr/local/hadoop'\n",
        "os.environ['SPARK_HOME'] = '/content/spark-3.4.3-bin-hadoop3'\n",
        "os.environ['PATH'] += ':/usr/local/hadoop/bin:/usr/local/hadoop/sbin:/content/spark-3.4.3-bin-hadoop3/bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "NRL9gllLF8GQ"
      },
      "outputs": [],
      "source": [
        "access_key = \"AKIAWZYVJD7JSRUFW5MR\"\n",
        "secret_key = \"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\"\n",
        "\n",
        "hadoop_core_site_path = '/usr/local/hadoop/etc/hadoop/core-site.xml'\n",
        "os.makedirs(os.path.dirname(hadoop_core_site_path), exist_ok=True)\n",
        "\n",
        "with open(hadoop_core_site_path, 'w') as f:\n",
        "    f.write(f\"\"\"\n",
        "    <configuration>\n",
        "        <property>\n",
        "            <name>fs.s3a.access.key</name>\n",
        "            <value>{access_key}</value>\n",
        "        </property>\n",
        "        <property>\n",
        "            <name>fs.s3a.secret.key</name>\n",
        "            <value>{secret_key}</value>\n",
        "        </property>\n",
        "        <property>\n",
        "            <name>fs.s3a.endpoint</name>\n",
        "            <value>s3.amazonaws.com</value>\n",
        "        </property>\n",
        "        <property>\n",
        "            <name>fs.s3a.impl</name>\n",
        "            <value>org.apache.hadoop.fs.s3a.S3AFileSystem</value>\n",
        "        </property>\n",
        "        <property>\n",
        "            <name>com.amazonaws.services.s3.enableV4</name>\n",
        "            <value>true</value>\n",
        "        </property>\n",
        "    </configuration>\n",
        "    \"\"\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4B-9k16mZC"
      },
      "source": [
        "## 3.5 Définition des PATH pour charger les images <br /> et enregistrer les résultats\n",
        "\n",
        "Dans cette version locale nous partons du principe que les données <br />\n",
        "sont stockées dans le même répertoire que le notebook.<br />\n",
        "Nous n'utilisons qu'un extrait de **300 images** à traiter dans cette <br />\n",
        "première version en local.<br />\n",
        "L'extrait des images à charger est stockée dans le dossier **Test1**.<br />\n",
        "Nous enregistrerons le résultat de notre traitement <br />\n",
        "dans le dossier \"**Results_Local**\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-1LzMdoL5i5s",
        "outputId": "ab0e2e50-92e5-42ba-9139-7b7a6a31397a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "PATH:        /content/drive/My Drive/Colab Notebooks/Projet_agritech\n",
            "PATH_Data:   /content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test\n",
            "PATH_Training: /content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Training\n",
            "PATH_Validation: /content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Validation\n",
            "PATH_Pepers: /content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Pepers\n",
            "PATH_Meta: /content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Meta\n",
            "PATH_Result: /content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Results\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# Monter Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Définir les chemins corrects\n",
        "PATH = \"/content/drive/My Drive/Colab Notebooks/Projet_agritech\"\n",
        "PATH_Data = os.path.join(PATH, \"data/Test\")\n",
        "PATH_Training = os.path.join(PATH, \"data/Training\")\n",
        "PATH_Validation = os.path.join(PATH, \"data/Validation\")\n",
        "PATH_Pepers = os.path.join(PATH, \"data/Pepers\")\n",
        "PATH_Meta = os.path.join(PATH, \"data/Meta\")\n",
        "PATH_Result = os.path.join(PATH, \"data/Results\")\n",
        "\n",
        "print('PATH:        ' + PATH)\n",
        "print('PATH_Data:   ' + PATH_Data)\n",
        "print('PATH_Training: ' + PATH_Training)\n",
        "print('PATH_Validation: ' + PATH_Validation)\n",
        "print('PATH_Pepers: ' + PATH_Pepers)\n",
        "print('PATH_Meta: ' + PATH_Meta)\n",
        "print('PATH_Result: ' + PATH_Result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eh6dx3nw3if"
      },
      "source": [
        "## 3.6 Création de la SparkSession\n",
        "\n",
        "L’application Spark est contrôlée grâce à un processus de pilotage (driver process) appelé **SparkSession**. <br />\n",
        "<u>Une instance de **SparkSession** est la façon dont Spark exécute les fonctions définies par l’utilisateur <br />\n",
        "dans l’ensemble du cluster</u>. <u>Une SparkSession correspond toujours à une application Spark</u>.\n",
        "\n",
        "<u>Ici nous créons une session spark en spécifiant dans l'ordre</u> :\n",
        " 1. un **nom pour l'application**, qui sera affichée dans l'interface utilisateur Web Spark \"**P8**\"\n",
        " 2. que l'application doit s'exécuter **localement**. <br />\n",
        "   Nous ne définissons pas le nombre de cœurs à utiliser (comme .master('local[4]) pour 4 cœurs à utiliser), <br />\n",
        "   nous utiliserons donc tous les cœurs disponibles dans notre processeur.<br />\n",
        " 3. une option de configuration supplémentaire permettant d'utiliser le **format \"parquet\"** <br />\n",
        "   que nous utiliserons pour enregistrer et charger le résultat de notre travail.\n",
        " 4. vouloir **obtenir une session spark** existante ou si aucune n'existe, en créer une nouvelle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v3o3jdf6S3M2"
      },
      "source": [
        "fs.s3a.impl--The implementation class of the S3A Filesystem\n",
        "\n",
        "fs.s3a.access.key--AWS access key ID used by S3A file system\n",
        "\n",
        "fs.s3a.secret.key--AWS secret key used by S3A file system\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "mRQ0rN5aoMVd"
      },
      "outputs": [],
      "source": [
        "!pip install -q findspark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "-uTNWYTvQG0I"
      },
      "outputs": [],
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"S3 Integration\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.2.1,com.amazonaws:aws-java-sdk-bundle:1.11.375\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", access_key) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", secret_key) \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.com.amazonaws.services.s3.enableV4\", \"true\") \\\n",
        "    .getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "FW_v2zqjhjgH"
      },
      "outputs": [],
      "source": [
        "sc = spark.sparkContext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "isNAXxpyiN0a",
        "outputId": "5e94ffdf-6003-4396-ffef-7d7bd6d93e53"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=S3 Integration>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cae5ceb96305:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>S3 Integration</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "sc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ckc2wdmbxFUx"
      },
      "source": [
        "## 3.7 Traitement des données\n",
        "\n",
        "<u>Dans la suite de notre flux de travail, <br />\n",
        "nous allons successivement</u> :\n",
        "1. Préparer nos données\n",
        "    1. Importer les images dans un dataframe **pandas UDF**\n",
        "    2. Associer aux images leur **label**\n",
        "    3. Préprocesser en **redimensionnant nos images pour <br />\n",
        "       qu'elles soient compatibles avec notre modèle**\n",
        "2. Préparer notre modèle\n",
        "    1. Importer le modèle **MobileNetV2**\n",
        "    2. Créer un **nouveau modèle** dépourvu de la dernière couche de MobileNetV2\n",
        "3. Définir le processus de chargement des images et l'application <br />\n",
        "   de leur featurisation à travers l'utilisation de pandas UDF\n",
        "3. Exécuter les actions d'extraction de features\n",
        "4. Enregistrer le résultat de nos actions\n",
        "5. Tester le bon fonctionnement en chargeant les données enregistrées\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tb4Q15dRxIpO"
      },
      "source": [
        "### 3.7.1 Chargement des données\n",
        "\n",
        "Les images sont chargées au format binaire, ce qui offre, <br />\n",
        "plus de souplesse dans la façon de prétraiter les images.\n",
        "\n",
        "Avant de charger les images, nous spécifions que nous voulons charger <br />\n",
        "uniquement les fichiers dont l'extension est **jpg**.\n",
        "\n",
        "Nous indiquons également de charger tous les objets possibles contenus <br />\n",
        "dans les sous-dossiers du dossier communiqué."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSsIV_ziiN9K",
        "outputId": "fc5d08aa-5aae-49a4-e66b-a16439767d40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------------------+------+--------------------+\n",
            "|                path|   modificationTime|length|             content|\n",
            "+--------------------+-------------------+------+--------------------+\n",
            "|file:/content/dri...|2024-05-28 22:16:46|125135|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|124785|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|123514|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|122958|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|122807|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|122654|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|122470|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|121883|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|121883|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|121530|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|121298|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|120070|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:48|119170|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:48|119016|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:49|118811|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|118684|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:49|117940|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:49|117825|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:46|117804|[FF D8 FF E0 00 1...|\n",
            "|file:/content/dri...|2024-05-28 22:16:49|117761|[FF D8 FF E0 00 1...|\n",
            "+--------------------+-------------------+------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Lire les fichiers d'images dans le dossier \"Test\"\n",
        "images = spark.read.format(\"binaryFile\") \\\n",
        "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
        "    .option(\"recursiveFileLookup\", \"true\") \\\n",
        "    .load(PATH_Data)\n",
        "\n",
        "images.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYLL8-YzxNOO"
      },
      "source": [
        "<u>Affichage des 5 premières images contenant</u> :\n",
        " - le path de l'image\n",
        " - la date et heure de sa dernière modification\n",
        " - sa longueur\n",
        " - son contenu encodé en valeur hexadécimal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNguOhPQxVH0",
        "outputId": "ebf8c427-3b03-4aab-f37f-e38f4c0e18d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- modificationTime: timestamp (nullable = true)\n",
            " |-- length: long (nullable = true)\n",
            " |-- content: binary (nullable = true)\n",
            " |-- label: string (nullable = true)\n",
            "\n",
            "None\n",
            "+---------------------------------------------------------------------------------------------+-----------+\n",
            "|path                                                                                         |label      |\n",
            "+---------------------------------------------------------------------------------------------+-----------+\n",
            "|file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_hit_1/r0_115.jpg|apple_hit_1|\n",
            "|file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_hit_1/r0_119.jpg|apple_hit_1|\n",
            "|file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_hit_1/r0_107.jpg|apple_hit_1|\n",
            "|file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_hit_1/r0_143.jpg|apple_hit_1|\n",
            "|file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_hit_1/r0_111.jpg|apple_hit_1|\n",
            "+---------------------------------------------------------------------------------------------+-----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "images = images.withColumn('label', element_at(split(images['path'], '/'),-2))\n",
        "print(images.printSchema())\n",
        "print(images.select('path','label').show(5,False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO00iXfPxc03"
      },
      "source": [
        "### 3.7.2 Préparation du modèle\n",
        "\n",
        "Je vais utiliser la technique du **transfert learning** pour extraire les features des images.<br />\n",
        "J'ai choisi d'utiliser le modèle **MobileNetV2** pour sa rapidité d'exécution comparée <br />\n",
        "à d'autres modèles comme *VGG16* par exemple.\n",
        "\n",
        "Pour en savoir plus sur la conception et le fonctionnement de MobileNetV2, <br />\n",
        "je vous invite à lire [cet article](https://towardsdatascience.com/review-mobilenetv2-light-weight-model-image-classification-8febb490e61c).\n",
        "\n",
        "<u>Voici le schéma de son architecture globale</u> :\n",
        "\n",
        "![Architecture de MobileNetV2](https://github.com/SatadruMukherjee/Data-Preprocessing-Models/blob/main/img/mobilenetv2_architecture.png?raw=1)\n",
        "\n",
        "Il existe une dernière couche qui sert à classer les images <br />\n",
        "selon 1000 catégories que nous ne voulons pas utiliser.<br />\n",
        "L'idée dans ce projet est de récupérer le **vecteur de caractéristiques <br />\n",
        "de dimensions (1,1,1280)** qui servira, plus tard, au travers d'un moteur <br />\n",
        "de classification à reconnaitre les différents fruits du jeu de données.\n",
        "\n",
        "Comme d'autres modèles similaires, **MobileNetV2**, lorsqu'on l'utilise <br />\n",
        "en incluant toutes ses couches, attend obligatoirement des images <br />\n",
        "de dimension (224,224,3). Nos images étant toutes de dimension (100,100,3), <br />\n",
        "nous devrons simplement les **redimensionner** avant de les confier au modèle.\n",
        "\n",
        "<u>Dans l'odre</u> :\n",
        " 1. Nous chargeons le modèle **MobileNetV2** avec les poids **précalculés** <br />\n",
        "    issus d'**imagenet** et en spécifiant le format de nos images en entrée\n",
        " 2. Nous créons un nouveau modèle avec:\n",
        "  - <u>en entrée</u> : l'entrée du modèle MobileNetV2\n",
        "  - <u>en sortie</u> : l'avant dernière couche du modèle MobileNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR1_A1Oaxd_l",
        "outputId": "0c1e4d6d-83a4-417c-95ec-484d68f63b24"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224.h5\n",
            "14536120/14536120 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Importation des bibliothèques nécessaires\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.models import Model\n",
        "# Chargement du modèle MobileNetV2 pré-entraîné\n",
        "model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "\n",
        "# Création un nouveau modèle sans la dernière couche (couche de classification)\n",
        "new_model = Model(inputs=model.input, outputs=model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IG8x_jp1xiMx"
      },
      "source": [
        "Affichage du résumé de notre nouveau modèle où nous constatons <br />\n",
        "que <u>nous récupérons bien en sortie un vecteur de dimension (1, 1, 1280)</u> :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CXSQfdD1xil4",
        "outputId": "7595667f-3b9e-45b7-adfe-74d1a5232a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 112, 112, 32)         864       ['input_1[0][0]']             \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 112, 112, 32)         128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 112, 112, 32)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 112, 112, 32)         288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 112, 112, 32)         128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 112, 112, 32)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 112, 112, 16)         512       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 112, 112, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 112, 112, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 112, 112, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 112, 112, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 113, 113, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 56, 56, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 56, 56, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 56, 56, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 56, 56, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 56, 56, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 56, 56, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 56, 56, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 56, 56, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 56, 56, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 56, 56, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 56, 56, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 57, 57, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 28, 28, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 28, 28, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 28, 28, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 28, 28, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 28, 28, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 28, 28, 32)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 28, 28, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 28, 28, 32)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_6_pad (ZeroPadding2D  (None, 29, 29, 192)          0         ['block_6_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_6_depthwise (Depthwi  (None, 14, 14, 192)          1728      ['block_6_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_6_depthwise_BN (Batc  (None, 14, 14, 192)          768       ['block_6_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_6_depthwise_relu (Re  (None, 14, 14, 192)          0         ['block_6_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_6_project (Conv2D)    (None, 14, 14, 64)           12288     ['block_6_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_6_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_6_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_7_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_6_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_7_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_7_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_7_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_7_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_7_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_7_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_7_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_7_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_7_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_7_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_7_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_7_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_7_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_7_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_7_add (Add)           (None, 14, 14, 64)           0         ['block_6_project_BN[0][0]',  \n",
            "                                                                     'block_7_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_8_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_7_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_8_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_8_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_8_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_8_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_8_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_8_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_8_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_8_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_8_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_8_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_8_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_8_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_8_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_8_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_8_add (Add)           (None, 14, 14, 64)           0         ['block_7_add[0][0]',         \n",
            "                                                                     'block_8_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_9_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_8_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_9_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_9_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_9_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_9_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_9_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_9_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_9_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_9_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_9_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_9_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_9_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_9_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_9_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_9_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_9_add (Add)           (None, 14, 14, 64)           0         ['block_8_add[0][0]',         \n",
            "                                                                     'block_9_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_10_expand (Conv2D)    (None, 14, 14, 384)          24576     ['block_9_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_10_expand_BN (BatchN  (None, 14, 14, 384)          1536      ['block_10_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_10_expand_relu (ReLU  (None, 14, 14, 384)          0         ['block_10_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_10_depthwise (Depthw  (None, 14, 14, 384)          3456      ['block_10_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_10_depthwise_BN (Bat  (None, 14, 14, 384)          1536      ['block_10_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_10_depthwise_relu (R  (None, 14, 14, 384)          0         ['block_10_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_10_project (Conv2D)   (None, 14, 14, 96)           36864     ['block_10_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_10_project_BN (Batch  (None, 14, 14, 96)           384       ['block_10_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_11_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_10_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_11_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_11_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_11_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_11_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_11_depthwise (Depthw  (None, 14, 14, 576)          5184      ['block_11_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_11_depthwise_BN (Bat  (None, 14, 14, 576)          2304      ['block_11_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_11_depthwise_relu (R  (None, 14, 14, 576)          0         ['block_11_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_11_project (Conv2D)   (None, 14, 14, 96)           55296     ['block_11_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_11_project_BN (Batch  (None, 14, 14, 96)           384       ['block_11_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_11_add (Add)          (None, 14, 14, 96)           0         ['block_10_project_BN[0][0]', \n",
            "                                                                     'block_11_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_12_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_11_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_12_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_12_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_12_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_12_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_12_depthwise (Depthw  (None, 14, 14, 576)          5184      ['block_12_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_12_depthwise_BN (Bat  (None, 14, 14, 576)          2304      ['block_12_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_12_depthwise_relu (R  (None, 14, 14, 576)          0         ['block_12_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_12_project (Conv2D)   (None, 14, 14, 96)           55296     ['block_12_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_12_project_BN (Batch  (None, 14, 14, 96)           384       ['block_12_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_12_add (Add)          (None, 14, 14, 96)           0         ['block_11_add[0][0]',        \n",
            "                                                                     'block_12_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_13_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_12_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_13_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_13_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_13_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_13_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_13_pad (ZeroPadding2  (None, 15, 15, 576)          0         ['block_13_expand_relu[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block_13_depthwise (Depthw  (None, 7, 7, 576)            5184      ['block_13_pad[0][0]']        \n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_13_depthwise_BN (Bat  (None, 7, 7, 576)            2304      ['block_13_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_13_depthwise_relu (R  (None, 7, 7, 576)            0         ['block_13_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_13_project (Conv2D)   (None, 7, 7, 160)            92160     ['block_13_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_13_project_BN (Batch  (None, 7, 7, 160)            640       ['block_13_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_14_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_13_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_14_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_14_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_14_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_14_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_14_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_14_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_14_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_14_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_14_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_14_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_14_project (Conv2D)   (None, 7, 7, 160)            153600    ['block_14_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_14_project_BN (Batch  (None, 7, 7, 160)            640       ['block_14_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_14_add (Add)          (None, 7, 7, 160)            0         ['block_13_project_BN[0][0]', \n",
            "                                                                     'block_14_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_15_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_14_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_15_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_15_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_15_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_15_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_15_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_15_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_15_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_15_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_15_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_15_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_15_project (Conv2D)   (None, 7, 7, 160)            153600    ['block_15_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_15_project_BN (Batch  (None, 7, 7, 160)            640       ['block_15_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_15_add (Add)          (None, 7, 7, 160)            0         ['block_14_add[0][0]',        \n",
            "                                                                     'block_15_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_16_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_15_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_16_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_16_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_16_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_16_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_16_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_16_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_16_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_16_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_16_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_16_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_16_project (Conv2D)   (None, 7, 7, 320)            307200    ['block_16_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_16_project_BN (Batch  (None, 7, 7, 320)            1280      ['block_16_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " Conv_1 (Conv2D)             (None, 7, 7, 1280)           409600    ['block_16_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " Conv_1_bn (BatchNormalizat  (None, 7, 7, 1280)           5120      ['Conv_1[0][0]']              \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " out_relu (ReLU)             (None, 7, 7, 1280)           0         ['Conv_1_bn[0][0]']           \n",
            "                                                                                                  \n",
            " global_average_pooling2d (  (None, 1280)                 0         ['out_relu[0][0]']            \n",
            " GlobalAveragePooling2D)                                                                          \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257984 (8.61 MB)\n",
            "Trainable params: 2223872 (8.48 MB)\n",
            "Non-trainable params: 34112 (133.25 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfBoE8lRxoiF"
      },
      "source": [
        "Tous les workeurs doivent pouvoir accéder au modèle ainsi qu'à ses poids. <br />\n",
        "Une bonne pratique consiste à charger le modèle sur le driver puis à diffuser <br />\n",
        "ensuite les poids aux différents workeurs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "dZLHO6xexpI2"
      },
      "outputs": [],
      "source": [
        "brodcast_weights = sc.broadcast(new_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "VLpK3UA7xuY3"
      },
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "\n",
        "    model = MobileNetV2(weights='imagenet', include_top=True, input_shape=(224, 224, 3))\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "    new_model = Model(inputs=model.input, outputs=model.layers[-2].output)\n",
        "    new_model.set_weights(brodcast_weights.value)\n",
        "    return new_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M6l5MGJNxzJH"
      },
      "source": [
        "### 3.7.3 Définition du processus de chargement des images et application <br/>de leur featurisation à travers l'utilisation de pandas UDF et de la réduction des dimensions PCA\n",
        "\n",
        "Ce notebook définit la logique par étapes, jusqu'à Pandas UDF.\n",
        "\n",
        "<u>L'empilement des appels est la suivante</u> :\n",
        "\n",
        "- Pandas UDF\n",
        "  - featuriser une série d'images pd.Series\n",
        "   - prétraiter une image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "4-BfsET77uwR"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import PCA\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.ml.feature import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "D-prupRZxzx3"
      },
      "outputs": [],
      "source": [
        "def nb_composante(dataframe, nb_comp=160):\n",
        "    pca = PCA(k=nb_comp, inputCol=\"features_scaled\", outputCol=\"features_pca\")\n",
        "    model_pca = pca.fit(dataframe)\n",
        "    varexpl = model_pca.explainedVariance * 100\n",
        "\n",
        "    plt.figure(figsize=(11, 7))\n",
        "    plt.bar(np.arange(len(varexpl)) + 1, varexpl)\n",
        "    cumSumVar = varexpl.cumsum()\n",
        "    plt.plot(np.arange(len(varexpl)) + 1, cumSumVar, c=\"red\", marker='o')\n",
        "    plt.axhline(y=95, linestyle=\"--\", color=\"green\", linewidth=1)\n",
        "    limit = 95\n",
        "    min_plans = np.argmax(cumSumVar >= limit) + 1\n",
        "    plt.axvline(x=min_plans, linestyle=\"--\", color=\"green\", linewidth=1)\n",
        "    plt.xlabel(\"Nombre de composantes principales\")\n",
        "    plt.ylabel(\"Pourcentage de variance expliquée\")\n",
        "    plt.title(\"{:.0f}% de la variance totale est expliquée par les {} premiers axes\".format(limit, min_plans))\n",
        "    plt.xticks(np.arange(0, nb_comp + 1, 10))\n",
        "    plt.xlim(left=0)\n",
        "    plt.show()\n",
        "\n",
        "    return min_plans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQi83slpx2KQ",
        "outputId": "679ca83a-8a8b-4c2d-99a6-a95bd1086dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def preprocess(content):\n",
        "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
        "    arr = img_to_array(img)\n",
        "    return preprocess_input(arr)\n",
        "\n",
        "def featurize_series(model, content_series):\n",
        "    input = np.stack(content_series.map(preprocess))\n",
        "    preds = model.predict(input)\n",
        "    output = [p.flatten() for p in preds]\n",
        "    return pd.Series(output)\n",
        "\n",
        "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
        "def featurize_udf(content_series_iter):\n",
        "    model = model_fn()\n",
        "    for content_series in content_series_iter:\n",
        "        yield featurize_series(model, content_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "5p46BP02x78t"
      },
      "outputs": [],
      "source": [
        "def preprocess_pca(dataframe):\n",
        "    transform_vecteur_dense = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
        "    dataframe = dataframe.withColumn('features_vectors', transform_vecteur_dense('features'))\n",
        "\n",
        "    scaler_std = StandardScaler(inputCol=\"features_vectors\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
        "    model_std = scaler_std.fit(dataframe)\n",
        "    dataframe = model_std.transform(dataframe)\n",
        "\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuJEmKBh9zQR"
      },
      "source": [
        "### 3.7.4 Exécution des actions d'extraction de features\n",
        "\n",
        "Les Pandas UDF, sur de grands enregistrements (par exemple, de très grandes images), <br />\n",
        "peuvent rencontrer des erreurs de type Out Of Memory (OOM).<br />\n",
        "Si vous rencontrez de telles erreurs dans la cellule ci-dessous, <br />\n",
        "essayez de réduire la taille du lot Arrow via 'maxRecordsPerBatch'\n",
        "\n",
        "Je n'utiliserai pas cette commande dans ce projet <br />\n",
        "et je laisse donc la commande en commentaire."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywNT5I4r93yh"
      },
      "source": [
        "Nous pouvons maintenant exécuter la featurisation sur l'ensemble de notre DataFrame Spark.<br />\n",
        "<u>REMARQUE</u> : Cela peut prendre beaucoup de temps, tout dépend du volume de données à traiter. <br />\n",
        "\n",
        "Notre jeu de données de **Test** contient **22819 images**. <br />\n",
        "Cependant, dans l'exécution en mode **local**, <br />\n",
        "nous <u>traiterons un ensemble réduit de **330 images**</u>."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "sfyXVMs_9z7B"
      },
      "outputs": [],
      "source": [
        "features_df = images.repartition(20).select(col(\"path\"),\n",
        "                                            col(\"label\"),\n",
        "                                            featurize_udf(\"content\").alias(\"features\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpeOoeulAVY_"
      },
      "source": [
        "<u>Rappel du PATH où seront inscrits les fichiers au format \"**parquet**\" <br />\n",
        "contenant nos résultats, à savoir, un DataFrame contenant 3 colonnes</u> :\n",
        " 1. Path des images\n",
        " 2. Label de l'image\n",
        " 3. Vecteur de caractéristiques de l'image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AYCVKddAaH4"
      },
      "source": [
        "<u>Enregistrement des données traitées au format \"**parquet**\"</u> :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "7RsricclAWid"
      },
      "outputs": [],
      "source": [
        "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbTWOodtAhTX"
      },
      "source": [
        "**Réduction de dimension PCA**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "FI60U2abAmRj"
      },
      "outputs": [],
      "source": [
        "# Pré-processing (vecteur dense, standardisation)\n",
        "df_pca = preprocess_pca(features_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "y9OSzR3bApH8",
        "outputId": "391430a5-5927-48e4-c90e-e09a537d5fa1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1100x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ],
      "source": [
        "# Nombre de composante expliquant 95% de la variance\n",
        "n_components = nb_composante(df_pca)\n",
        "n_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "sBznj0RoAsPw"
      },
      "outputs": [],
      "source": [
        "# Entrainement de l'algorithme\n",
        "pca = PCA(k=n_components, inputCol='features_scaled', outputCol='features_pca')\n",
        "model_pca = pca.fit(df_pca)\n",
        "\n",
        "# Transformation des images sur les k premières composantes\n",
        "df_reduit = model_pca.transform(df_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "I9F_-PpUAu55"
      },
      "outputs": [],
      "source": [
        "# Write the result after PCA to a parquet file\n",
        "df_reduit.write.mode(\"overwrite\").parquet(PATH_Result + \"/pca_results\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m53DWAY7AydZ"
      },
      "source": [
        "## 3.8 Chargement des données enregistrées et validation du résultat\n",
        "\n",
        "<u>On charge les données fraichement enregistrées dans un **DataFrame Pandas**</u> :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "9oaHAy1TAzPa"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(PATH_Result, engine='pyarrow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "mJRGPqdTA5C8"
      },
      "outputs": [],
      "source": [
        "df_pca=spark.read.parquet(PATH_Result + \"/pca_results\", engine='pyarrow')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SzqDM1KA7fl",
        "outputId": "918392b6-15db-4348-f26a-9496e0214501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- label: string (nullable = true)\n",
            " |-- features: array (nullable = true)\n",
            " |    |-- element: float (containsNull = true)\n",
            " |-- features_vectors: vector (nullable = true)\n",
            " |-- features_scaled: vector (nullable = true)\n",
            " |-- features_pca: vector (nullable = true)\n",
            "\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(df_pca.printSchema())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTc8q5-Z2Fmf",
        "outputId": "5bacd17a-4830-4e59-88ef-35a9a5a06713"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[path: string, label: string, features: array<float>, features_vectors: vector, features_scaled: vector, features_pca: vector]"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ],
      "source": [
        "df_pca"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Nahy20iyA9hB",
        "outputId": "126423fd-245d-4235-c130-3ad3de5f0a92"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                path        label  \\\n",
              "0  file:/content/drive/My Drive/Colab Notebooks/P...  apple_hit_1   \n",
              "1  file:/content/drive/My Drive/Colab Notebooks/P...  apple_hit_1   \n",
              "2  file:/content/drive/My Drive/Colab Notebooks/P...  apple_hit_1   \n",
              "3  file:/content/drive/My Drive/Colab Notebooks/P...  apple_hit_1   \n",
              "4  file:/content/drive/My Drive/Colab Notebooks/P...  apple_hit_1   \n",
              "\n",
              "                                            features  \n",
              "0  [0.26334843, 0.16236944, 1.3314823, 0.0, 0.470...  \n",
              "1  [1.0761654, 0.4118579, 2.2395241, 0.0, 0.10062...  \n",
              "2  [0.22258626, 0.22375964, 1.0577579, 0.00630717...  \n",
              "3  [0.120855175, 1.231432, 0.08040661, 0.0, 0.264...  \n",
              "4  [0.018150378, 0.062381968, 1.0725708, 0.0, 0.2...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-1f50bcae-ce2f-486d-bda1-b4a3f58669e2\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "      <th>features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td>apple_hit_1</td>\n",
              "      <td>[0.26334843, 0.16236944, 1.3314823, 0.0, 0.470...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td>apple_hit_1</td>\n",
              "      <td>[1.0761654, 0.4118579, 2.2395241, 0.0, 0.10062...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td>apple_hit_1</td>\n",
              "      <td>[0.22258626, 0.22375964, 1.0577579, 0.00630717...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td>apple_hit_1</td>\n",
              "      <td>[0.120855175, 1.231432, 0.08040661, 0.0, 0.264...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td>apple_hit_1</td>\n",
              "      <td>[0.018150378, 0.062381968, 1.0725708, 0.0, 0.2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1f50bcae-ce2f-486d-bda1-b4a3f58669e2')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1f50bcae-ce2f-486d-bda1-b4a3f58669e2 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1f50bcae-ce2f-486d-bda1-b4a3f58669e2');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-12f74666-a1ee-4b0e-b779-7a1521935462\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-12f74666-a1ee-4b0e-b779-7a1521935462')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-12f74666-a1ee-4b0e-b779-7a1521935462 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6220,\n  \"fields\": [\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3110,\n        \"samples\": [\n          \"file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_pink_lady_1/r0_131.jpg\",\n          \"file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_braeburn_1/r0_251.jpg\",\n          \"file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_red_delicios_1/r0_15.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 24,\n        \"samples\": [\n          \"cucumber_1\",\n          \"apple_pink_lady_1\",\n          \"apple_hit_1\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 68
        }
      ],
      "source": [
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wao5ncG_BK1w"
      },
      "source": [
        "<u>On valide que la dimension du vecteur de caractéristiques des images est bien de dimension 1280</u> :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JWxUr2zMBJWv",
        "outputId": "0db99410-66f6-43a7-df9d-8a5c84a19d1c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1280,)"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ],
      "source": [
        "df.loc[0,'features'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWZZZ0v7BSYA",
        "outputId": "5157dba8-f70f-4571-f263-4e6a4d93c3cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dimension après réduction PCA: 1\n"
          ]
        }
      ],
      "source": [
        "# Sélectionner la colonne \"pca_features\" et la renommer en \"features\"\n",
        "df_pca_features = df_pca.select(col(\"features_pca\").alias(\"features\"))\n",
        "first_row = df_pca_features.first()\n",
        "dimension = len(first_row[\"features\"])\n",
        "print(\"Dimension après réduction PCA:\", dimension)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mnZU6H_RBU1z",
        "outputId": "e1d953f3-f272-4f9d-c909-99531ede85ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['path',\n",
              " 'label',\n",
              " 'features',\n",
              " 'features_vectors',\n",
              " 'features_scaled',\n",
              " 'features_pca']"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ],
      "source": [
        "df_pca.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mSjNHLU4BYuv",
        "outputId": "a18419ea-99cf-45d2-b9ac-1c44715adb06"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3110"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ],
      "source": [
        "df_pca.count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWgNj58sOiJR"
      },
      "source": [
        "# 4. Déploiement de la solution sur le cloud\n",
        "\n",
        "Maintenant que nous avons vérifié que notre solution fonctionne, <br />\n",
        "il est temps de la <u>déployer à plus grande échelle sur un vrai cluster de machines</u>.\n",
        "\n",
        "**Attention**, *je travaille sous Linux avec une version Ubuntu, <br />\n",
        "les commandes décrites ci-dessous sont donc réalisées <br />\n",
        "exclusivement dans cet environnement.*\n",
        "\n",
        "<u>Plusieurs contraintes se posent</u> :\n",
        " 1. Quel prestataire de Cloud choisir ?\n",
        " 2. Quelles solutions de ce prestataire adopter ?\n",
        " 3. Où stocker nos données ?\n",
        " 4. Comment configurer nos outils dans ce nouvel environnement ?\n",
        "\n",
        "## 4.1 Choix du prestataire cloud : AWS\n",
        "\n",
        "Le prestataire le plus connu et qui offre à ce jour l'offre <br />\n",
        "la plus large dans le cloud computing est **Amazon Web Services** (AWS).<br />\n",
        "Certaines de leurs offres sont parfaitement adaptées à notre problématique <br />\n",
        "et c'est la raison pour laquelle j'utiliserai leurs services.\n",
        "\n",
        "L'objectif premier est de pouvoir, grâce à AWS, <u>louer de la puissance de calcul à la demande</u>. <br />\n",
        "L'idée étant de pouvoir, quel que soit la charge de travail, <br />\n",
        "obtenir suffisamment de puissance de calcul pour pouvoir traiter nos images, <br />\n",
        "même si le volume de données venait à fortement augmenter.\n",
        "\n",
        "De plus, la capacité d'utiliser cette puissance de calcul à la demande <br />\n",
        "permet de diminuer drastiquement les coûts si l'on compare les coûts d'une location <br />\n",
        "de serveur complet sur une durée fixe (1 mois, 1 année par exemple).\n",
        "\n",
        "## 4.2 Choix de la solution technique : EMR\n",
        "\n",
        "<u>Plusieurs solutions s'offre à nous</u> :\n",
        "1. Solution **IAAS** (Infrastructure AS A Service)\n",
        " - Dans cette configuration **AWS** met à notre disposition des serveurs vierges <br />\n",
        "   sur lequel nous avons un accès en administrateur, ils sont nommés **instance EC2**.<br />\n",
        "   Pour faire simple, nous pouvons avec cette solution reproduire pratiquement <br />\n",
        "   à l'identique la solution mis en œuvre en local sur notre machine.<br />\n",
        "   <u>On installe nous-même l'intégralité des outils puis on exécute notre script</u> :\n",
        "  - Installation de **Spark**, **Java** etc.\n",
        "  - Installation de **Python** (via Anaconda par exemple)\n",
        "  - Utilisation de **Colab**\n",
        "  - Installation des **librairies complémentaires**\n",
        "  - Il faudra bien évidement veiller à **implémenter les librairies\n",
        "    nécessaires à toutes les machines (workers) du cluster**\n",
        "  - <u>Avantages</u> :\n",
        "      - Liberté totale de mise en œuvre de la solution\n",
        "      - Facilité de mise en œuvre à partir d'un modèle qui s'exécute en local sur une machine Linux\n",
        "  - <u>Inconvénients</u> :\n",
        "      - Cronophage\n",
        "          - Nécessité d'installer et de configurer toute la solution\n",
        "      - Possible problèmes techniques à l'installation des outils (des problématiques qui <br />\n",
        "        n'existaient pas en local sur notre machine peuvent apparaitre sur le serveur EC2)\n",
        "      - Solution non pérenne dans le temps, il faudra veiller à la mise à jour des outils <br />\n",
        "        et éventuellement devoir réinstaller Spark, Java etc.\n",
        "2. Solution **PAAS** (Plateforme As A Service)\n",
        " - **AWS** fournit énormément de services différents, dans l'un de ceux-là <br />\n",
        "   il existe une offre qui permet de louer des **instances EC2** <br />\n",
        "   avec des applications préinstallées et configurées : il s'agit du **service EMR**.\n",
        " - **Spark** y sera déjà installé\n",
        " - Possibilité de demander l'installation de **Tensorflow** ainsi que **Colab**\n",
        " - Possibilité d'indiquer des **packages complémentaires** à installer <br />\n",
        "   à l'initialisation du serveur **sur l'ensemble des machines du cluster**.\n",
        " - <u>Avantages</u> :\n",
        "     - Facilité de mise en œuvre\n",
        "         - Il suffit de très peu de configuration pour obtenir <br />\n",
        "           un environnement parfaitement fonctionnel\n",
        "     - Rapidité de mise en œuvre\n",
        "         - Une fois la première configuration réalisée, il est très facile <br />\n",
        "           et très rapide de recréer des clusters à l'identique qui seront <br />\n",
        "           disponibles presque instantanément (le temps d'instancier les <br />\n",
        "           serveurs soit environ 15/20 minutes)\n",
        "     - Solutions matérielless et logicielles optimisées par les ingénieurs d'AWS\n",
        "         - On sait que les versions installées vont fonctionner <br />\n",
        "           et que l'architecture proposée est optimisée\n",
        "     - Stabilité de la solution\n",
        "    - Solution évolutive\n",
        "        Il est facile d’obtenir à chaque nouvelle instanciation une version à jour <br />\n",
        "        de chaque package, en étant garanti de leur compatibilité avec le reste de l’environnement.\n",
        "  - Plus sécurisé\n",
        "\t- Les éventuels patchs de sécurité seront automatiquement mis à jour <br />\n",
        "      à chaque nouvelle instanciation du cluster EMR.\n",
        " - <u>Inconvénients</u> :\n",
        "     - Peut-être un certain manque de liberté sur la version des packages disponibles ? <br />\n",
        "       Même si je n'ai pas constaté ce problème.\n",
        "   \n",
        "\n",
        "Je retiens la solution **PAAS** en choisissant d'utiliser <br />\n",
        "le service **EMR** d'Amazon Web Services.<br />\n",
        "Je la trouve plus adaptée à notre problématique et permet <br />\n",
        "une mise en œuvre qui soit à la fois plus rapide et <br />\n",
        "plus efficace que la solution IAAS.\n",
        "\n",
        "## 4.3 Choix de la solution de stockage des données : Amazon S3\n",
        "\n",
        "<u>Amazon propose une solution très efficace pour la gestion du stockage des données</u> : **Amazon S3**. <br />\n",
        "S3 pour Amazon Simple Storage Service.\n",
        "\n",
        "Il pourrait être tentant de stocker nos données sur l'espace alloué par le serveur **EC2**, <br />\n",
        "mais si nous ne prenons aucune mesure pour les sauvegarder ensuite sur un autre support, <br />\n",
        "<u>les données seront perdues</u> lorsque le serveur sera résilié (on résilie le serveur lorsqu'on <br />\n",
        "ne s'en sert pas pour des raisons de coût).<br />\n",
        "De fait, si l'on décide d'utiliser l'espace disque du serveur EC2 il faudra imaginer <br />\n",
        "une solution pour sauvegarder les données avant la résiliation du serveur.\n",
        "De plus, nous serions exposés à certaines problématiques si nos données venaient à <br />\n",
        "**saturer** l'espace disponible de nos serveurs (ralentissements, disfonctionnements).\n",
        "\n",
        "<u>Utiliser **Amazon S3** permet de s'affranchir de toutes ces problématiques</u>. <br />\n",
        "L'espace disque disponible est **illimité**, et il est **indépendant de nos serveurs EC2**. <br />\n",
        "L'accès aux données est **très rapide** car nous restons dans l'environnement d'AWS <br />\n",
        "et nous prenons soin de <u>choisir la même région pour nos serveurs **EC2** et **S3**</u>.\n",
        "\n",
        "De plus, comme nous le verrons <u>il est possible d'accéder aux données sur **S3** <br />\n",
        "    de la même manière que l'on **accède aux données sur un disque local**</u>.<br />\n",
        "Nous utiliserons simplement un **PATH au format s3://...** .\n",
        "\n",
        "## 4.4 Configuration de l'environnement de travail\n",
        "\n",
        "La première étape est d'installer et de configurer [**AWS Cli**](https://aws.amazon.com/fr/cli/),<br />\n",
        "il s'agit de l'**interface en ligne de commande d'AWS**.<br />\n",
        "Elle nous permet d'**interagir avec les différents services d'AWS**, comme **S3** par exemple.\n",
        "\n",
        "Pour pouvoir utiliser **AWS Cli**, il faut le configurer en créant préalablement <br />\n",
        "un utilisateur à qui on donnera les autorisations dont nous aurons besoin.<br />\n",
        "Dans ce projet il faut que l'utilisateur ait à minima un contrôle total sur le service S3.\n",
        "\n",
        "<u>La gestion des utilisateurs et de leurs droits s'effectue via le service **AMI**</u> d'AWS.\n",
        "\n",
        "Une fois l'utilisateur créé et ses autorisations configurées nous créons une **paire de clés** <br />\n",
        "qui nous permettra de nous **connecter sans à avoir à devoir saisir systématiquement notre login/mot de passe**.<br />\n",
        "\n",
        "Il faut également configurer l'**accès SSH** à nos futurs serveurs EC2. <br />\n",
        "Ici aussi, via un système de clés qui nous dispense de devoir nous authentifier \"à la main\" à chaque connexion.\n",
        "\n",
        "Toutes ses étapes de configuration sont parfaitement décrites <br />\n",
        "dans le cours du projet: [Réalisez des calculs distribués sur des données massives / Découvrez Amazon Web Services](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308686-decouvrez-amazon-web-services#/id/r-4355822)\n",
        "\n",
        "## 4.5 Upload de nos données sur S3\n",
        "\n",
        "Nos outils sont configurés. <br />\n",
        "Il faut maintenant uploader nos données de travail sur Amazon S3.\n",
        "\n",
        "Ici aussi les étapes sont décrites avec précision <br />\n",
        "dans le cours [Réalisez des calculs distribués sur des données massives / Stockez des données sur S3](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308691-stockez-des-donnees-sur-s3)\n",
        "\n",
        "Je décide de n'uploader que les données contenues dans le dossier **Test** du [jeu de données du projet](https://www.kaggle.com/moltean/fruits/download)\n",
        "\n",
        "\n",
        "La première étape consiste à **créer un bucket sur S3** <br />\n",
        "dans lequel nous uploaderons les données du projet:\n",
        "- **aws s3 mbs3://oc-calculddist/Projet_agritech/data/**\n",
        "\n",
        "On vérifie que le bucket à bien été créé\n",
        "- **aws s3 ls**\n",
        " - Si le nom du bucket s'affiche alors c'est qu'il a été correctement créé.\n",
        "\n",
        "On copie ensuite le contenu du dossier \"**Test**\" <br />\n",
        "dans un répertoire \"**Test**\" sur notre bucket \"**Projet_agritech**\":\n",
        "1. On se place à l'intérieur du répertoire **Test**\n",
        "2. **aws sync . s3://oc-calculddist/Projet_agritech/data/**\n",
        "\n",
        "La commande **sync** est utile pour synchroniser deux répertoires.\n",
        "\n",
        "<u>Nos données du projet sont maintenant disponibles sur Amazon S3</u>.\n",
        "\n",
        "## 4.6 Configuration du serveur EMR\n",
        "\n",
        "Une fois encore, le cours [Réalisez des calculs distribués sur des données massives / Déployez un cluster de calculs distribués](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues) <br /> détaille l'essentiel des étapes pour lancer un cluster avec **EMR**.\n",
        "\n",
        "<u>Je détaillerai ici les étapes particulières qui nous permettent <br />\n",
        "de configurer le serveur selon nos besoins</u> :\n",
        "\n",
        "1. Cliquez sur Créer un cluster\n",
        "![Créer un cluster](img/EMR_creer.png)\n",
        "2. Cliquez sur Accéder aux options avancées\n",
        "![Créer un cluster](img/EMR_options_avancees.png)\n",
        "\n",
        "### 4.6.1 Étape 1 : Logiciels et étapes\n",
        "\n",
        "#### 4.6.1.1 Configuration des logiciels\n",
        "\n",
        "<u>Sélectionnez les packages dont nous aurons besoin comme dans la capture d'écran</u> :\n",
        "1. Nous sélectionnons la dernière version d'**EMR**, soit la version **6.3.0** au moment où je rédige ce document\n",
        "2. Nous cochons bien évidement **Hadoop** et **Spark** qui seront préinstallés dans leur version la plus récente\n",
        "3. Nous aurons également besoin de **TensorFlow** pour importer notre modèle et réaliser le **transfert learning**\n",
        "4. Nous travaillerons enfin avec un **notebook Jupyter** via l'application **JupyterHub**<br />\n",
        " - Comme nous le verrons dans un instant nous allons <u>paramétrer l'application afin que les notebooks</u>, <br />\n",
        "   comme le reste de nos données de travail, <u>soient enregistrés directement sur S3</u>.\n",
        "![Créer un cluster](img/EMR_configuration_logiciels.png)\n",
        "\n",
        "#### 4.6.1.2 Modifier les paramètres du logiciel\n",
        "\n",
        "<u>Paramétrez la persistance des notebooks créés et ouvert via JupyterHub</u> :\n",
        "- On peut à cette étape effectuer des demandes de paramétrage particulières sur nos applications. <br />\n",
        "  L'objectif est, comme pour le reste de nos données de travail, <br />\n",
        "  d'éviter toutes les problématiques évoquées précédemment. <br />\n",
        "  C'est l'objectif à cette étape, <u>nous allons enregistrer <br />\n",
        "  et ouvrir les notebooks</u> non pas sur l'espace disque de  l'instance EC2 (comme <br />\n",
        "  ce serait le cas dans la configuration par défaut de JupyterHub) mais <br />\n",
        "  <u>directement sur **Amazon S3**</u>.\n",
        "- <u>deux solutions sont possibles pour réaliser cela</u> :\n",
        " 1. Créer un **fichier de configuration JSON** que l'on **upload sur S3** et on indique ensuite le chemin d’accès au fichier JSON\n",
        " 2. Rentrez directement la configuration au format JSON\n",
        "\n",
        "J'ai personnellement créé un fichier JSON lors de la création de ma première instance EMR, <br />\n",
        "puis lorsqu'on décide de cloner notre serveur pour en recréer un facilement à l'identique, <br />\n",
        "la configuration du fichier JSON se retrouve directement copié comme dans la capture ci-dessous.\n",
        "\n",
        "<u>Voici le contenu de mon fichier JSON</u> :  [{\"classification\":\"jupyter-s3-conf\",\"properties\":{\"s3.persistence.bucket\":\"p8-data\",\"s3.persistence.enabled\":\"true\"}}]\n",
        " Appuyez ensuite sur \"**Suivant**\"\n",
        "![Modifier les paramètres du logiciel](img/EMR_parametres_logiciel.png)\n",
        "\n",
        "### 4.6.2 Étape 2 : Matériel\n",
        "\n",
        "A cette étape, laissez les choix par défaut. <br />\n",
        "<u>L'important ici est la sélection de nos instances</u> :\n",
        "\n",
        "1. je choisi les instances de type **M5** qui sont des **instances de type équilibrés**\n",
        "2. je choisi le type **xlarge** qui est l'instance la **moins onéreuse disponible**\n",
        " [Plus d'informations sur les instances M5 Amazon EC2](https://aws.amazon.com/fr/ec2/instance-types/m5/)\n",
        "3. Je sélectionne **1 instance Maître** (le driver) et **2 instances Principales** (les workeurs) <br />\n",
        "   soit **un total de 3 instance EC2**.\n",
        "![Choix du materiel](img/EMR_materiel.png)\n",
        "\n",
        "### 4.6.3 Étape 3 : Paramètres de cluster généraux\n",
        "\n",
        "#### 4.6.3.1 Options générales\n",
        "<u>La première chose à faire est de donner un nom au cluster</u> :<br />\n",
        "*J'ai également décoché \"Protection de la résiliation\" pour des raisons pratiques.*\n",
        "    \n",
        "![Nom du Cluster](img/EMR_nom_cluster.png)\n",
        "\n",
        "#### 4.6.3.2 Actions d'amorçage\n",
        "\n",
        "Nous allons à cette étape **choisir les packages manquants à installer** et qui <br />\n",
        "nous serons utiles dans l'exécution de notre notebook.<br />\n",
        "<u>L'avantage de réaliser cette étape maintenant est que les packages <br />\n",
        "installés le seront sur l'ensemble des machines du cluster</u>.\n",
        "\n",
        "La procédure pour créer le fichier **bootstrap** qui contient <br />\n",
        "l'ensemble des instructions permettant d'installer tous <br />\n",
        "les packages dont nous aurons besoin est expliqué dans <br />\n",
        "le cours [Réalisez des calculs distribués sur des données massives / Bootstrapping](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356490)\n",
        "\n",
        "Nous créons donc un fichier nommé \"**bootstrap-emr.sh**\" que nous <u>uploadons <br />\n",
        "sur S3</u>(je l’installe à la racine de mon **bucket \"p8-data\"**) et nous l'ajoutons <br />\n",
        "comme indiqué dans la capture d'écran ci-dessous:\n",
        "![Actions d'amorcage](img/EMR_amorcage.png)\n",
        "\n",
        "Voici le contenu du fichier **bootstrap-emr.sh**<br />\n",
        "Comme on peut le constater il s'agit simplement de commande \"**pip install**\" <br />\n",
        "pour **installer les bibliothèques manquantes** comme réalisé en local.<br />\n",
        "Une fois encore, <u>il est nécessaire de réaliser ces actions à cette étape</u> <br />\n",
        "pour que <u>les packages soient installés sur l'ensemble des machines du cluster</u> <br />\n",
        "et non pas uniquement sur le driver, comme cela serait le cas si nous exécutions <br />\n",
        "ces commandes directement dans le notebook JupyterHub ou dans la console EMR (connecté au driver).\n",
        "![Contenu du fichier bootstrap](img/EMR_bootstrap.png)\n",
        "\n",
        "**setuptools** et **pip** sont mis à jour pour éviter une problématique <br />\n",
        "avec l'installation du package **pyarrow**.<br />\n",
        "**Pandas** a eu droit à une mise à jour majeur (1.3.0) il y a moins d'une semaine <br />\n",
        "au moment de la rédaction de ce notebook, et la nouvelle version de **Pandas** <br />\n",
        "nécessite une version plus récente de **Numpy** que la version installée par <br />\n",
        "défaut (1.16.5) à l'initialisation des instances **EC2**. <u>Il ne semble pas <br />\n",
        "possible d'imposer une autre version de Numpy que celle installé par <br />\n",
        "défaut</u> même si on force l'installation d'une version récente de **Numpy** <br />\n",
        "(en tout cas, ni simplement ni intuitivement).<br />\n",
        "La mise à jour étant très récente <u>la version de **Numpy** n'est pas encore <br />\n",
        "mise à jour sur **EC2**</u> mais on peut imaginer que ce sera le cas très rapidement <br />\n",
        "et il ne sera plus nécessaire d'imposer une version spécifique de **Pandas**.<br />\n",
        "En attendant, je demande <u>l'installation de l'avant dernière version de **Pandas (1.2.5)**</u>\n",
        "\n",
        "On clique ensuite sur ***Suivant***\n",
        "\n",
        "### 4.6.4 Étape 4 : Sécurité\n",
        "\n",
        "#### 4.6.4.1 Options de sécurité\n",
        "\n",
        "A cette étape nous sélectionnons la **paire de clés EC2** créé précédemment. <br />\n",
        "Elle nous permettra de se connecter en **ssh** à nos **instances EC2** <br />\n",
        "sans avoir à entrer nos login/mot de passe.<br />\n",
        "On laisse les autres paramètres par défaut. <br />\n",
        "Et enfin, on clique sur \"***Créer un cluster***\"\n",
        "\n",
        "![EMR Sécurité](img/EMR_securite.png)\n",
        "\n",
        "## 4.7 Instanciation du serveur\n",
        "\n",
        "Il ne nous reste plus qu'à attendre que le serveur soit prêt. <br />\n",
        "Cette étape peut prendre entre **15 et 20 minutes**.\n",
        "\n",
        "<u>Plusieurs étapes s'enchaîne, on peut suivre l'avancé du statut du **cluster EMR**</u> :\n",
        "\n",
        "![Instanciation étape 1](img/EMR_instanciation_01.png)\n",
        "![Instanciation étape 2](img/EMR_instanciation_02.png)\n",
        "![Instanciation étape 3](img/EMR_instanciation_03.png)\n",
        "\n",
        "<u>Lorsque le statut affiche en vert: \"**En attente**\" cela signifie que l'instanciation <br />\n",
        "s'est bien déroulée et que notre serveur est prêt à être utilisé</u>.\n",
        "\n",
        "## 4.8 Création du tunnel SSH à l'instance EC2 (Maître)\n",
        "\n",
        "### 4.8.1 Création des autorisations sur les connexions entrantes\n",
        "\n",
        "<u>Nous souhaitons maintenant pouvoir accéder à nos applications</u> :\n",
        " - **JupyterHub** pour l'exécution de notre notebook\n",
        " - **Serveur d'historique Spark** pour le suivi de l'exécution <br />\n",
        "   des tâches de notre script lorsqu'il sera lancé\n",
        "\n",
        "Cependant, <u>ces applications ne sont accessibles que depuis le réseau local du driver</u>, <br />\n",
        "et pour y accéder nous devons **créer un tunnel SSH vers le driver**.\n",
        "\n",
        "Par défaut, ce driver se situe derrière un firewall qui bloque l'accès en SSH. <br />\n",
        "<u>Pour ouvrir le port 22 qui correspond au port sur lequel écoute le serveur SSH, <br />\n",
        "il faut modifier le **groupe de sécurité EC2 du driver**</u>.\n",
        "\n",
        "Cette étape est décrite dans le cours [Réalisez des calculs distribués sur des données massives / Lancement d'une application à partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308696-deployez-un-cluster-de-calculs-distribues#/id/r-4356512):\n",
        "\n",
        "*Il faudra que l'on se connecte en SSH au driver de notre cluster. <br />\n",
        "Par défaut, ce driver se situe derrière un firewall qui bloque l'accès en SSH. <br />\n",
        "Pour ouvrir le port 22 qui correspond au port sur lequel écoute le serveur SSH, <br />\n",
        "il faut modifier le groupe de sécurité EC2 du driver. Sur la page de la console <br />\n",
        "consacrée à EC2, dans l'onglet \"Réseau et sécurité\", cliquez sur \"Groupes de sécurité\". <br />\n",
        "Vous allez devoir modifier le groupe de sécurité d’ElasticMapReduce-Master. <br />\n",
        "Dans l'onglet \"Entrant\", ajoutez une règle SSH dont la source est \"N'importe où\" <br />\n",
        "(ou \"Mon IP\" si vous disposez d'une adresse IP fixe).*\n",
        "\n",
        "![Configuration autorisation ports entrants pour ssh](img/EMR_config_ssh_01.png)\n",
        "\n",
        "<u>Une fois cette étape réalisée vous devriez avoir une configuration semblable à la mienne</u> :\n",
        "\n",
        "![Configuration ssh terminée](img/EMR_config_ssh_02.png)\n",
        "\n",
        "### 4.8.2 Création du tunnel ssh vers le Driver\n",
        "\n",
        "On peut maintenant établir le **tunnel SSH** vers le **Driver**. <br />\n",
        "Pour cela on récupère les informations de connexion fournis par Amazon <br />\n",
        "depuis la page du service EMR / Cluster / onglet Récapitulatif en <br />\n",
        "cliquant sur \"**Activer la connexion Web**\"\n",
        "\n",
        "![Activer la connexion Web](img/EMR_tunnel_ssh_01.png)\n",
        "\n",
        "<u>On récupère ensuite la commande fournis par Amazon pour **établir le tunnel SSH**</u> :\n",
        "\n",
        "![Récupérer la commande pour établir le tunnel ssh](img/EMR_tunnel_ssh_02.png)\n",
        "\n",
        "<u>Dans mon cas, la commande ne fonctionne pas tel</u> quel et j'ai du **l'adapter à ma configuration**. <br />\n",
        "La **clé ssh** se situe dans un dossier \"**.ssh**\" elle-même située dans <br />\n",
        "mon **répertoire personnel** dont le symbole est, sous Linux, identifié par un tilde \"**~**\".\n",
        "\n",
        "Ayant suivi le cours [Réalisez des calculs distribués sur des données massives / Lancement d'une application à partir du driver](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives) <br />\n",
        "j'ai choisi d'utiliser le port **5555** au lieu du **8157**, même si le choix n'est pas très important.<br />\n",
        "    j'ai également rencontré un <u>problème de compatibilité</u> avec <br />\n",
        "l'argument \"**-N**\" (liste des arguments et leur significations <br />\n",
        "disponibles [ici](https://explainshell.com/explain?cmd=ssh+-L+-N+-f+-l+-D)) j'ai décidé de simplement le supprimer.\n",
        "\n",
        "<u>Finalement, j'utilise la commande suivante dans un terminal pour établir <br />\n",
        "    mon tunnel ssh (seul l'URL change d'une instance à une autre)</u> : <br />\n",
        "\"**ssh -i ~/.ssh/p8-ec2.pem -D 5555 hadoop@ec2-35-180-91-39.eu-west-3.compute.amazonaws.com**\"\n",
        "\n",
        "<u>On inscrit \"**yes**\" pour valider la connexion et si <br />\n",
        "    la connexion est établit on obtient le résultat suivant</u> :\n",
        "\n",
        "![Création du tunnel SSH](img/EMR_connexion_ssh_01.png)\n",
        "\n",
        "Nous avons **correctement établi le tunnel ssh avec le driver** sur le port \"5555\".\n",
        "\n",
        "### 4.8.3 Configuration de FoxyProxy\n",
        "\n",
        "Une dernière étape est nécessaire pour accéder à nos applications, <br />\n",
        "en demandant à notre navigateur d'emprunter le tunnel ssh.<br />\n",
        "J'utilise pour cela **FoxyProxy**.\n",
        "[Une fois encore, vous pouvez utiliser le cours pour le configurer](https://openclassrooms.com/fr/courses/4297166-realisez-des-calculs-distribues-sur-des-donnees-massives/4308701-realisez-la-maintenance-dun-cluster#/id/r-4356554).\n",
        "\n",
        "Sinon, ouvrez la configuration de **FoxyProxy** et <u>cliquez sur **Ajouter**</u> en haut à gauche <br />\n",
        "puis renseigner les éléments comme dans la capture ci-dessous :\n",
        "\n",
        "![Configuration FoxyProxy Etape 1](img/EMR_foxyproxy_config_01.png)\n",
        "\n",
        "<u>On obtient le résultat ci-dessous</u> :\n",
        "\n",
        "![Configuration FoxyProxy Etape 2](img/EMR_foxyproxy_config_02.png)\n",
        "\n",
        "\n",
        "### 4.8.4 Accès aux applications du serveur EMR via le tunnel ssh\n",
        "\n",
        "\n",
        "<u>Avant d'établir notre **tunnel ssh** nous avions ça</u> :\n",
        "\n",
        "![avant tunnel ssh](img/EMR_tunnel_ssh_avant.png)\n",
        "\n",
        "<u>On active le **tunnel ssh** comme vu précédemment puis on demande <br />\n",
        "à notre navigateur de l'utiliser avec **FoxyProxy**</u> :\n",
        "\n",
        "![FoxyProxy activation](img/EMR_foxyproxy_activation.png)\n",
        "\n",
        "<u>On peut maintenant s'apercevoir que plusieurs applications nous sont accessibles</u> :\n",
        "\n",
        "![avant tunnel ssh](img/EMR_tunnel_ssh_apres.png)\n",
        "\n",
        "## 4.9 Connexion au notebook JupyterHub\n",
        "\n",
        "Pour se connecter à **JupyterHub** en vue d'exécuter notre **notebook**, <br />\n",
        "il faut commencer par <u>cliquer sur l'application **JupyterHub**</u> apparu <br />\n",
        "depuis que nous avons configuré le **tunnel ssh** et **foxyproxy** sur <br />\n",
        "notre navigateur (actualisez la page si ce n’est pas le cas).\n",
        "\n",
        "![Démarrage de JupyterHub](img/EMR_jupyterhub_connexion_01.png)\n",
        "\n",
        "On passe les éventuels avertissements de sécurité puis <br />\n",
        "nous arrivons sur une page de connexion.\n",
        "    \n",
        "<u>On se connecte avec les informations par défaut</u> :\n",
        " - <u>login</u>: **jovyan**\n",
        " - <u>password</u>: **jupyter**\n",
        "\n",
        "![Connexion à JupyterHub](img/EMR_jupyterhub_connexion_02.png)\n",
        "\n",
        "Nous arrivons ensuite dans un dossier vierge de notebook.<br />\n",
        "Il suffit d'en créer un en cliquant sur \"**New**\" en haut à droite.\n",
        "\n",
        "![Liste et création des notebook](img/EMR_jupyterhub_creer_notebooks.png)\n",
        "\n",
        "Il est également possible d'en <u>uploader un directement dans notre **bucket S3**</u>.\n",
        "\n",
        "Grace à la <u>**persistance** paramétrée à l'instanciation du cluster <br />\n",
        "nous sommes actuellement dans l'arborescence de notre **bucket S3**</u>\n",
        "\n",
        "![Notebook stockés sur S3](img/EMR_jupyterhub_S3.png)\n",
        "\n",
        "Je décide d'**importer un notebook déjà rédigé en local directement <br />\n",
        "sur S3** et je l'ouvre depuis **l'interface JupyterHub**.\n",
        "\n",
        "## 4.10 Exécution du code\n",
        "\n",
        "Je décide d'exécuter cette partie du code depuis **JupyterHub hébergé sur notre cluster EMR**.<br />\n",
        "Pour ne pas alourdir inutilement les explications du **notebook**, je ne réexpliquerai pas les étapes communes <br />\n",
        "que nous avons déjà vues dans la première partie où l'on a exécuté le code localement sur notre machine virtuelle Ubuntu.\n",
        "\n",
        "<u>Avant de commencer</u>, il faut s'assurer d'utiliser le **kernel pyspark**.\n",
        "\n",
        "**En utilisant ce kernel, une session spark est créé à l'exécution de la première cellule**. <br />\n",
        "Il n'est donc **plus nécessaire d'exécuter le code \"spark = (SparkSession ...\"** comme lors <br />\n",
        "de l'exécution de notre colab en local"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install findspark\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORECnJQf34QQ",
        "outputId": "db175475-6ffd-4cc5-bf13-6a6f2dcf9656"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Téléchargement de Spark\n",
        "!wget https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "\n",
        "# Extraction de l'archive\n",
        "!tar -xvf spark-3.4.0-bin-hadoop3.tgz\n",
        "\n",
        "# Déplacement du répertoire Spark dans un emplacement approprié\n",
        "!mv spark-3.4.0-bin-hadoop3 /content/spark-3.4.0-bin-hadoop3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jfyl9rlg4JJu",
        "outputId": "658dc45d-b817-4a71-8281-3ca9dcebd96c"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-10 13:41:20--  https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
            "Resolving archive.apache.org (archive.apache.org)... 65.108.204.189, 2a01:4f9:1a:a084::2\n",
            "Connecting to archive.apache.org (archive.apache.org)|65.108.204.189|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 388407094 (370M) [application/x-gzip]\n",
            "Saving to: ‘spark-3.4.0-bin-hadoop3.tgz.2’\n",
            "\n",
            "spark-3.4.0-bin-had 100%[===================>] 370.41M  9.44MB/s    in 31s     \n",
            "\n",
            "2024-06-10 13:41:52 (12.0 MB/s) - ‘spark-3.4.0-bin-hadoop3.tgz.2’ saved [388407094/388407094]\n",
            "\n",
            "spark-3.4.0-bin-hadoop3/\n",
            "spark-3.4.0-bin-hadoop3/data/\n",
            "spark-3.4.0-bin-hadoop3/data/streaming/\n",
            "spark-3.4.0-bin-hadoop3/data/streaming/AFINN-111.txt\n",
            "spark-3.4.0-bin-hadoop3/data/graphx/\n",
            "spark-3.4.0-bin-hadoop3/data/graphx/followers.txt\n",
            "spark-3.4.0-bin-hadoop3/data/graphx/users.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/als/\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/als/test.data\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/als/sample_movielens_ratings.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/gmm_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_libsvm_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_svm_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_fpgrowth.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/kmeans_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/pic_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/streaming_kmeans_data_test.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_linear_regression_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/ridge-data/\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/ridge-data/lpsa.data\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_binary_classification_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/pagerank_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_isotonic_regression_libsvm_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_kmeans_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_lda_libsvm_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/license.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/multi-channel/\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/multi-channel/chr30.4.184.jpg\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA_alpha_60.png\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/multi-channel/grayscale.jpg\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/multi-channel/BGRA.png\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/license.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/kittens/\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/kittens/DP802813.jpg\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/kittens/54893.jpg\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/kittens/not-image.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/kittens/DP153539.jpg\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/images/origin/kittens/29.5.a_b_EGDP022204.jpg\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_lda_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_movielens_data.txt\n",
            "spark-3.4.0-bin-hadoop3/data/mllib/sample_multiclass_classification_data.txt\n",
            "spark-3.4.0-bin-hadoop3/LICENSE\n",
            "spark-3.4.0-bin-hadoop3/jars/\n",
            "spark-3.4.0-bin-hadoop3/jars/zookeeper-jute-3.6.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/derby-10.14.2.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-shims-0.23-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/oro-2.0.8.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jline-2.14.6.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-buffer-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/libthrift-0.12.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/json-1.8.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-events-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/avro-ipc-1.11.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jodd-core-3.5.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-dataformat-yaml-2.14.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-node-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-graphx_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-compiler-3.1.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/javolution-5.5.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jersey-hk2-2.36.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-native-epoll-4.1.87.Final-linux-x86_64.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/opencsv-2.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/orc-core-1.8.3-shaded-protobuf.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-native-epoll-4.1.87.Final-linux-aarch_64.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/zjsonpatch-0.3.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-collections-3.2.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/json4s-scalap_2.12-3.7.0-M11.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-core-asl-1.9.13.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jersey-container-servlet-2.36.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-annotations-2.14.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/JLargeArrays-1.5.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/antlr-runtime-3.5.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/arpack-3.0.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/annotations-17.0.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-mllib-local_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jul-to-slf4j-2.0.6.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/guava-14.0.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/mesos-1.4.3-shaded-protobuf.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/orc-shims-1.8.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-storage-api-2.8.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-datatype-jsr310-2.14.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/xbean-asm9-shaded-4.22.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/breeze_2.12-2.1.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/zookeeper-3.6.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/parquet-column-1.12.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-mllib_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spire-macros_2.12-0.17.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-compress-1.22.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/ST4-4.0.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hadoop-client-api-3.3.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/arrow-vector-11.0.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/curator-framework-2.13.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/antlr4-runtime-4.9.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/log4j-slf4j2-impl-2.19.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jersey-client-2.36.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-databind-2.14.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/scala-library-2.12.17.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-networking-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-llap-common-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/scala-compiler-2.12.17.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/bonecp-0.8.0.RELEASE.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-mesos_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/snappy-java-1.1.9.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-codec-http2-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-common-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spire-platform_2.12-0.17.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/parquet-jackson-1.12.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/lz4-java-1.8.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/curator-client-2.13.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jdo-api-3.0.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-serde-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/httpcore-4.4.16.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/transaction-api-1.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-handler-proxy-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/univocity-parsers-2.9.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-flowcontrol-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-lang-2.6.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-collections4-4.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-sketch_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.87.Final-osx-x86_64.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/metrics-jvm-4.2.15.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jsr305-3.0.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-network-shuffle_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/py4j-0.10.9.7.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jta-1.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/HikariCP-2.5.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-catalyst_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/scala-reflect-2.12.17.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-pool-1.5.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/JTransforms-3.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-math3-3.6.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/flatbuffers-java-1.12.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/rocksdbjni-7.9.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/okio-1.15.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jersey-server-2.36.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/json4s-core_2.12-3.7.0-M11.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/datanucleus-core-4.1.17.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/threeten-extra-1.7.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spire_2.12-0.17.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-core-2.14.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hadoop-client-runtime-3.3.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/osgi-resource-locator-1.0.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/cats-kernel_2.12-2.1.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/paranamer-2.8.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/pickle-1.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-all-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-streaming_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spire-util_2.12-0.17.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/janino-3.1.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/curator-recipes-2.13.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jersey-common-2.36.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-common-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hk2-locator-2.6.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jakarta.annotation-api-1.3.5.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jpam-1.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-native-kqueue-4.1.87.Final-osx-aarch_64.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-client-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/orc-mapreduce-1.8.3-shaded-protobuf.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/metrics-core-4.2.15.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/datanucleus-rdbms-4.1.19.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-repl_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jakarta.inject-2.6.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/snakeyaml-1.33.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/minlog-1.3.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/httpclient-4.5.14.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/parquet-format-structures-1.12.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hk2-utils-2.6.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-cli-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-codec-http-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-httpclient-okhttp-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-module-scala_2.12-2.14.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/audience-annotations-0.5.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/arrow-memory-netty-11.0.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-common-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-classes-epoll-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-certificates-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/parquet-hadoop-1.12.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-core-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-service-rpc-3.1.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-kubernetes_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-admissionregistration-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-codec-1.15.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/objenesis-3.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jackson-mapper-asl-1.9.13.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-crypto-1.1.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-network-common_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-shims-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/aircompressor-0.21.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/xz-1.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-policy-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-rbac-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/scala-parser-combinators_2.12-2.1.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/arrow-memory-core-11.0.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-apiextensions-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jersey-container-servlet-core-2.36.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/protobuf-java-2.5.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-logging-1.1.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-cli-1.5.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-unsafe_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/log4j-1.2-api-2.19.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jakarta.validation-api-2.0.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-kvstore_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/blas-3.0.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/stax-api-1.0.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-autoscaling-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-dbcp-1.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/RoaringBitmap-0.9.38.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/slf4j-api-2.0.6.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/parquet-encoding-1.12.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-codec-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-batch-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/activation-1.1.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/aopalliance-repackaged-2.6.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/metrics-jmx-4.2.15.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/ivy-2.5.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-sql_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jakarta.servlet-api-4.0.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-extensions-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-metastore-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/tink-1.7.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/gson-2.2.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/zstd-jni-1.5.2-5.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-shims-scheduler-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-core_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-native-unix-common-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hadoop-shaded-guava-1.1.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/compress-lzf-1.1.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/arrow-format-11.0.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/logging-interceptor-3.12.12.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-beeline-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-launcher_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jakarta.ws.rs-api-2.1.6.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-apps-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/json4s-jackson_2.12-3.7.0-M11.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-tags_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/okhttp-3.12.12.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jcl-over-slf4j-2.0.6.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-metrics-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/istack-commons-runtime-3.0.8.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-jdbc-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/log4j-api-2.19.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/javax.jdo-3.2.0-m3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-codec-socks-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/leveldbjni-all-1.8.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/stream-2.9.6.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/avro-mapred-1.11.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/avro-1.11.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/libfb303-0.9.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/super-csv-2.2.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-storageclass-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-resolver-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jaxb-runtime-2.3.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/scala-xml_2.12-2.1.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-coordination-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-scheduling-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/chill-java-0.10.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-client-api-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-handler-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/breeze-macros_2.12-2.1.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/algebra_2.12-2.0.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-lang3-3.12.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/scala-collection-compat_2.12-2.7.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-io-2.11.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-yarn_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/jakarta.xml.bind-api-2.3.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kryo-shaded-4.0.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hk2-api-2.6.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-hive_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-shims-common-2.3.9.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hive-exec-2.3.9-core.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-gatewayapi-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/spark-hive-thriftserver_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/metrics-graphite-4.2.15.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/chill_2.12-0.10.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/shims-0.9.38.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/datanucleus-api-jdo-4.2.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/hadoop-yarn-server-web-proxy-3.3.4.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/netty-transport-classes-kqueue-4.1.87.Final.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/metrics-json-4.2.15.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/json4s-ast_2.12-3.7.0-M11.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/dropwizard-metrics-hadoop-metrics2-reporter-0.1.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/kubernetes-model-discovery-6.4.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/javassist-3.25.0-GA.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/joda-time-2.12.2.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/log4j-core-2.19.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/arpack_combined_all-0.1.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/commons-text-1.10.0.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/parquet-common-1.12.3.jar\n",
            "spark-3.4.0-bin-hadoop3/jars/lapack-3.0.3.jar\n",
            "spark-3.4.0-bin-hadoop3/R/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/sparkr.zip\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/NAMESPACE\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/worker/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/worker/worker.R\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/worker/daemon.R\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/profile/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/profile/general.R\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/profile/shell.R\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/tests/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/tests/testthat/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/tests/testthat/test_basic.R\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/R/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/R/SparkR.rdx\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/R/SparkR.rdb\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/R/SparkR\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/package.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/features.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/vignette.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/nsInfo.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/Rd.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/hsearch.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/Meta/links.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/html/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/html/R.css\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/html/00Index.html\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/doc/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.Rmd\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.R\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/doc/sparkr-vignettes.html\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/doc/index.html\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/INDEX\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/DESCRIPTION\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/help/\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/help/SparkR.rdx\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/help/aliases.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/help/SparkR.rdb\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/help/paths.rds\n",
            "spark-3.4.0-bin-hadoop3/R/lib/SparkR/help/AnIndex\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/decom.sh\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/R/Dockerfile\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/bindings/python/Dockerfile\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/entrypoint.sh\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/dockerfiles/spark/Dockerfile\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/py_container_checks.py\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/decommissioning.py\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/autoscale.py\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/pyfiles.py\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/worker_memory_check.py\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/python_executable_check.py\n",
            "spark-3.4.0-bin-hadoop3/kubernetes/tests/decommissioning_cleanup.py\n",
            "spark-3.4.0-bin-hadoop3/examples/\n",
            "spark-3.4.0-bin-hadoop3/examples/jars/\n",
            "spark-3.4.0-bin-hadoop3/examples/jars/scopt_2.12-3.7.1.jar\n",
            "spark-3.4.0-bin-hadoop3/examples/jars/spark-examples_2.12-3.4.0.jar\n",
            "spark-3.4.0-bin-hadoop3/examples/src/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scripts/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scripts/getGpusResources.sh\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/streaming/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/streaming/structured_network_wordcount.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/RSparkSQLExample.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/dataframe.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/data-manipulation.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/decisionTree.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/logit.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/kstest.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/gaussianMixture.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/fmRegressor.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/naiveBayes.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/ml.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/lda.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/kmeans.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/isoreg.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/lm_with_elastic_net.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/als.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/fpm.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/glm.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/bisectingKmeans.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/svmLinear.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/prefixSpan.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/powerIterationClustering.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/fmClassifier.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/randomForest.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/gbt.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/survreg.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/r/ml/mlp.R\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/GroupByTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPageRank.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SQLDataSourceExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedScalar.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCountWindowed.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredSessionization.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKerberizedKafkaWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredComplexSessionization.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredNetworkWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/streaming/StructuredKafkaWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedUntypedAggregation.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/UserDefinedTypedAggregation.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/RDDRelation.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SimpleTypedAggregator.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/SparkSQLExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/jdbc/ExampleJdbcConnectionProvider.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/sql/hive/SparkHiveExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SkewedGroupByTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkALS.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalPi.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/CustomReceiver.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewStream.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/clickstream/PageViewGenerator.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RecoverableNetworkWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKafkaWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StreamingExamples.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/QueueStream.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/SqlNetworkWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/StatefulNetworkWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/DirectKerberizedKafkaWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/HdfsWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/RawNetworkGrep.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/streaming/NetworkWordCount.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ExceptionHandlingTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkRemoteFileTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DFSReadWriteTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkTC.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/BroadcastTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkKMeans.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/HdfsTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalLR.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalKMeans.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/LiveJournalPageRank.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ConnectedComponentsExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/ComprehensiveExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/AggregateMessagesExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SSSPExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/SynthBenchmark.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/PageRankExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/TriangleCountingExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/graphx/Analytics.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkHdfsLR.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SparkSessionExtensionsTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithoutLoader.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/AgeExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/extensions/SessionExtensionsWithLoader.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalFileLR.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkLR.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MultiBroadcastTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LogQuery.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SimpleSkewedGroupByTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/LocalALS.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/pythonconverters/AvroConverters.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PCAExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ALSExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSquareTestExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/AFTSurvivalRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RobustScalerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CountVectorizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestClassifierExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnaryTransformerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSizeHintExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NGramExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PolynomialExpansionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RFormulaExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MaxAbsScalerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BisectingKMeansExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaCrossValidationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GeneralizedLinearRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IndexToStringExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/Word2VecExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DCTExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMRegressorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/UnivariateFeatureSelectorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TokenizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketedRandomProjectionLSHExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VarianceThresholdSelectorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/IsotonicRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ImputerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PrefixSpanExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DeveloperApiExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PipelineExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ChiSqSelectorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NormalizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ModelSelectionViaTrainValidationSplitExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FPGrowthExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/PowerIterationClusteringExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StringIndexerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorAssemblerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneVsRestExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LDAExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearSVCExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FMClassifierExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/CorrelationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/OneHotEncoderExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeClassifierExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LinearRegressionWithElasticNetExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/LogisticRegressionSummaryExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/KMeansExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BinarizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/QuantileDiscretizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StandardScalerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MulticlassLogisticRegressionWithElasticNetExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GradientBoostedTreeRegressorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/BucketizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DataFrameExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinHashLSHExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/ElementwiseProductExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MultilayerPerceptronClassifierExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/DecisionTreeClassificationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/StopWordsRemoverExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SQLTransformerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/InteractionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/NaiveBayesExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorSlicerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/FeatureHasherExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/TfIdfExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/RandomForestRegressorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GBTExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/MinMaxScalerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/SummarizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/VectorIndexerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/EstimatorTransformerParamExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/ml/GaussianMixtureExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/MiniReadWriteTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/AccumulatorMetricsTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/SparkPi.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/DriverSubmissionTest.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CosineSimilarity.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LatentDirichletAllocationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRunner.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingKMeansExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLogisticRegression.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DenseKMeans.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TFIDFExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LogisticRegressionWithLBFGSExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnyPCA.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/TallSkinnySVD.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LBFGSExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BisectingKMeansExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StratifiedSamplingExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultiLabelMetricsExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SampledRDDs.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestClassificationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SimpleFPGrowth.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Word2VecExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/CorrelationsExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnSourceVectorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PMMLModelExportExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/IsotonicRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomRDDGeneration.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVDExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PrefixSpanExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassification.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ChiSqSelectorExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NormalizerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/FPGrowthExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PowerIterationClusteringExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RankingMetricsExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/LDAExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MulticlassMetricsExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RecommendationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SummaryStatisticsExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MovieLensALS.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KMeansExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AbstractParams.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SparseNaiveBayes.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StandardScalerExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/MultivariateSummarizer.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/RandomForestRegressionExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/SVMWithSGDExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/AssociationRulesExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/ElementwiseProductExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingTestExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/KernelDensityEstimationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostingClassificationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/DecisionTreeClassificationExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/BinaryClassificationMetricsExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/PCAOnRowMatrixExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/NaiveBayesExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GradientBoostedTreesRunner.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/Correlations.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/HypothesisTestingKolmogorovSmirnovTestExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/scala/org/apache/spark/examples/mllib/GaussianMixtureExample.scala\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/streaming/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_windowed.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_sessionization.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/streaming/__init__,py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_kafka_wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/streaming/structured_network_wordcount_session_window.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/datasource.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/basic.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/arrow.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sql/hive.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/kmeans.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/sql_network_wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/stateful_network_wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/network_wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/queue_stream.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/network_wordjoinsentiments.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/hdfs_wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/streaming/recoverable_network_wordcount.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/avro_inputformat.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/logistic_regression.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/status_api_demo.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/pagerank.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/parquet_inputformat.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/transitive_closure.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/als.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/sort.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/dct_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/normalizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/random_forest_classifier_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/standard_scaler_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/estimator_transformer_param_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/binarizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/dataframe_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/aft_survival_regression.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/kmeans_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/imputer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/cross_validator.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/bucketed_random_projection_lsh_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/univariate_feature_selector_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/rformula_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/onehot_encoder_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/chi_square_test_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/vector_size_hint_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/linear_regression_with_elastic_net.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/decision_tree_classification_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/__init__,py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/feature_hasher_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/count_vectorizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/tokenizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/word2vec_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/bucketizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/stopwords_remover_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/polynomial_expansion_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/elementwise_product_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/pca_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/bisecting_k_means_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/robust_scaler_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/vector_assembler_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/quantile_discretizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/tf_idf_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_classifier_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/fm_classifier_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/linearsvc.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/min_max_scaler_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/correlation_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/generalized_linear_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/multiclass_logistic_regression_with_elastic_net.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/sql_transformer.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/index_to_string_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/vector_indexer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/multilayer_perceptron_classification.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/summarizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/vector_slicer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/logistic_regression_summary_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/pipeline_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/n_gram_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/naive_bayes_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/logistic_regression_with_elastic_net.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/fpgrowth_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/variance_threshold_selector_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/als_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/fm_regressor_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/train_validation_split.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/decision_tree_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/random_forest_regressor_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/prefixspan_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/gradient_boosted_tree_regressor_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/min_hash_lsh_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/gaussian_mixture_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/string_indexer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/isotonic_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/interaction_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/power_iteration_clustering_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/chisq_selector_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/lda_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/max_abs_scaler_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/ml/one_vs_rest_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/pi.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/normalizer_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/logistic_regression_with_lbfgs_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/kmeans.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/standard_scaler_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/word2vec.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_classification_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/random_forest_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/latent_dirichlet_allocation_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/regression_metrics_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_kolmogorov_smirnov_test_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/decision_tree_classification_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/logistic_regression.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/linear_regression_with_sgd_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/hypothesis_testing_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/multi_class_metrics_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/k_means_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/binary_classification_metrics_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/word2vec_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/recommendation_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/ranking_metrics_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/summary_statistics_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/elementwise_product_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/bisecting_k_means_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_model.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/multi_label_metrics_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/sampled_rdds.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/tf_idf_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/streaming_k_means_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/stratified_sampling_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/streaming_linear_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/kernel_density_estimation_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/correlations.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/random_rdd_generation.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/naive_bayes_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/fpgrowth_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/decision_tree_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/random_forest_classification_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/svm_with_sgd_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/gaussian_mixture_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/isotonic_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/svd_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/power_iteration_clustering_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/pca_rowmatrix_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/correlations_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/python/mllib/gradient_boosting_regression_example.py\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedUntypedAggregation.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredNetworkWordCountWindowed.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredComplexSessionization.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKerberizedKafkaWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredSessionization.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/streaming/JavaStructuredKafkaWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedScalar.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaUserDefinedTypedAggregation.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSQLDataSourceExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/JavaSparkSQLExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/sql/hive/JavaSparkHiveExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaStatusTrackerDemo.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecoverableNetworkWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKafkaWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaQueueStream.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaDirectKerberizedKafkaWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaCustomReceiver.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaStatefulNetworkWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaNetworkWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaRecord.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/streaming/JavaSqlNetworkWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaSparkPi.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaWordCount.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaTC.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaLogQuery.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaPageRank.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/JavaHdfsLR.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaKMeansExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketizerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSlicerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaALSExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearSVCExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorIndexerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVarianceThresholdSelectorExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCountVectorizerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinHashLSHExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStandardScalerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSQLTransformerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaUnivariateFeatureSelectorExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLabeledDocument.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionSummaryExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTokenizerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLinearRegressionWithElasticNetExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDCTExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestRegressorExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNormalizerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaWord2VecExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPolynomialExpansionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRFormulaExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPowerIterationClusteringExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaSummarizerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPipelineExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaQuantileDiscretizerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBinarizerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeClassifierExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMinMaxScalerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMulticlassLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNaiveBayesExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorSizeHintExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBisectingKMeansExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLDAExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMaxAbsScalerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRandomForestClassifierExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMRegressorExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaRobustScalerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaLogisticRegressionWithElasticNetExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaVectorAssemblerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaAFTSurvivalRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneVsRestExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaInteractionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGradientBoostedTreeRegressorExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIndexToStringExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPCAExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSquareTestExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFMClassifierExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaOneHotEncoderExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaImputerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaBucketedRandomProjectionLSHExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStringIndexerExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaChiSqSelectorExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaCrossValidationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGeneralizedLinearRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaCorrelationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaElementwiseProductExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaMultilayerPerceptronClassifierExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaNGramExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaTfIdfExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaIsotonicRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFeatureHasherExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaStopWordsRemoverExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaEstimatorTransformerParamExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaDocument.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaModelSelectionViaTrainValidationSplitExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaPrefixSpanExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaFPGrowthExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/ml/JavaGaussianMixtureExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLBFGSExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKMeansExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStratifiedSamplingExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMulticlassClassificationMetricsExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestClassificationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaAssociationRulesExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVDExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRecommendationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeClassificationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaMultiLabelClassificationMetricsExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPowerIterationClusteringExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaKernelDensityEstimationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaNaiveBayesExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSVMWithSGDExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBisectingKMeansExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLatentDirichletAllocationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaBinaryClassificationMetricsExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaLogisticRegressionWithLBFGSExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaALS.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaStreamingTestExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSimpleFPGrowth.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPCAExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingClassificationExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRandomForestRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaHypothesisTestingKolmogorovSmirnovTestExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaChiSqSelectorExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaDecisionTreeRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaSummaryStatisticsExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaRankingMetricsExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaElementwiseProductExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaCorrelationsExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaIsotonicRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaPrefixSpanExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGradientBoostingRegressionExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/java/org/apache/spark/examples/mllib/JavaGaussianMixtureExample.java\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/users.parquet\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/META-INF/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/META-INF/services/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.jdbc.JdbcConnectionProvider\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/META-INF/services/org.apache.spark.sql.SparkSessionExtensionsProvider\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/user.avsc\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/employees.json\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/dir1/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/dir1/file1.parquet\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/dir1/file3.json\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/dir1/dir2/\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/dir1/dir2/file2.parquet\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/users.avro\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/kv1.txt\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/people.json\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/people.txt\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/users.orc\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/full_user.avsc\n",
            "spark-3.4.0-bin-hadoop3/examples/src/main/resources/people.csv\n",
            "spark-3.4.0-bin-hadoop3/conf/\n",
            "spark-3.4.0-bin-hadoop3/conf/metrics.properties.template\n",
            "spark-3.4.0-bin-hadoop3/conf/workers.template\n",
            "spark-3.4.0-bin-hadoop3/conf/spark-defaults.conf.template\n",
            "spark-3.4.0-bin-hadoop3/conf/log4j2.properties.template\n",
            "spark-3.4.0-bin-hadoop3/conf/spark-env.sh.template\n",
            "spark-3.4.0-bin-hadoop3/conf/fairscheduler.xml.template\n",
            "spark-3.4.0-bin-hadoop3/NOTICE\n",
            "spark-3.4.0-bin-hadoop3/yarn/\n",
            "spark-3.4.0-bin-hadoop3/yarn/spark-3.4.0-yarn-shuffle.jar\n",
            "spark-3.4.0-bin-hadoop3/python/\n",
            "spark-3.4.0-bin-hadoop3/python/.gitignore\n",
            "spark-3.4.0-bin-hadoop3/python/setup.cfg\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/install.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/dataframe.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/plan.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/_typing.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/relations_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/catalog_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/common_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/expressions_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/commands_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/example_plugins_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/commands_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/expressions_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/types_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/relations_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/common_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/example_plugins_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/catalog_pb2.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/types_pb2.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/proto/base_pb2_grpc.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/window.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/readwriter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/column.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/session.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/udf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/types.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/conf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/group.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/catalog.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/client.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/expressions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/connect/conversion.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/dataframe.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/sql_formatter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/observation.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/listener.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/readwriter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/query.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/streaming/state.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/avro/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/avro/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/avro/functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_function.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_group.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_catalog.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_arrow.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_plan.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_conf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_udf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_dataframe.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_arrow_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_udf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_column.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_grouped_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_types.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_datasources.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_column.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_connect_basic.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_client.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_errors.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_serde.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_readwriter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/connect/test_parity_pandas_cogrouped_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_conf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_errors.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_session.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_group.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/streaming/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/streaming/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming_listener.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/streaming/test_streaming.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_arrow.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_dataframe.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_catalog.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_arrow_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_column.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_types.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_udf_profiler.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_datasources.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_typehints.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_grouped_map_with_state.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_grouped_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_grouped_agg.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_window.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_cogrouped_map.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_scalar.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/pandas/test_pandas_udf_typehints_with_future_annotations.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_readwriter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_pandas_sqlmetrics.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_serde.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/typing/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_dataframe.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_column.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_udf.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_functions.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_readwriter.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/typing/test_session.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_arrow_python_udf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_udf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/tests/test_context.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/context.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/window.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/readwriter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/column.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/_typing.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/session.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/protobuf/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/protobuf/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/protobuf/functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/serializers.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/map_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/typehints.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/__init__.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/__init__.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/series.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/_typing/protocols/frame.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/functions.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/types.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/group_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/pandas/conversion.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/udf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/types.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/conf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/group.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/sql/catalog.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/serializers.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/connectutils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/mllibutils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/pandasutils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/mlutils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/streamingutils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/testing/sqlutils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/dstream.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/kinesis.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/tests/test_listener.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/tests/test_context.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/tests/test_kinesis.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/tests/test_dstream.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/listener.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/context.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/streaming/util.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/profile.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/tests/test_resources.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/requests.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resource/information.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/_globals.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/statcounter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/cloudpickle/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/cloudpickle/compat.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/cloudpickle/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/cloudpickle/cloudpickle_fast.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_statcounter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_conf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_taskcontext.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_profiler.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_rddbarrier.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_util.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_shuffle.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_rdd.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_broadcast.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_serializers.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_memory_profiler.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_worker.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_readwrite.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_pin_thread.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_appsubmit.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_install_spark.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/typing/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/typing/test_rdd.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/typing/test_core.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/typing/test_context.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/typing/test_resultiterable.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_context.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_rddsampler.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_join.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_stage_sched.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/tests/test_daemon.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/taskcontext.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/resultiterable.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/rdd.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/context.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/traceback_utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/status.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/daemon.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/files.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/version.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/__pycache__/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/__pycache__/install.cpython-38.pyc\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/shuffle.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/_typing.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/py.typed\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/broadcast.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/find_spark_home.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/resample.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/_typing.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/sql_formatter.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/typedef/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/typedef/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/typedef/typehints.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/plot/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/plot/core.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/plot/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/plot/matplotlib.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/plot/plotly.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/series.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/supported_api_gen.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_indexing.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_config.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_frame_spark.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_window.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_series.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_groupby_slow.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_plotly.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_plotly.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_series_plot_matplotlib.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/plot/test_frame_plot_matplotlib.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_extension.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_scalars.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_conversion.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_series_string.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_sql.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_stats.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_spark_io.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_repr.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_series_conversion.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_slow.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_date_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_datetime_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_timedelta_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_null_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_udt_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_string_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_binary_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_categorical_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_boolean_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_num_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_complex_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/test_base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/data_type_ops/testing_utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_generic_functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_series_datetime.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_rolling.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_groupby.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_dataframe_slow.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_namespace.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_default_index.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_reshape.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_ewm.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_spark_functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames_groupby_expanding.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_typedef.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_csv.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_expanding.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_category.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_timedelta.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/indexes/test_datetime.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_categorical.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_internal.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_ops_on_diff_frames.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_rolling.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_indexops_spark.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_numpy_compat.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/tests/test_resample.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/exceptions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/resample.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/series.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/scalars.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/window.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/frame.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/common.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/indexes.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/groupby.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/missing/general_functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/window.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/config.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/timedelta_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/null_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/complex_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/num_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/categorical_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/udt_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/binary_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/boolean_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/string_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/datetime_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/data_type_ops/date_ops.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/frame.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/categorical.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/strings.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/numpy_compat.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/datetimes.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/sql_processor.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/usage_logging/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/usage_logging/usage_logger.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/usage_logging/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/correlation.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/accessors.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/extensions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/multi.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/timedelta.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/datetimes.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/numeric.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexes/category.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/internal.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/generic.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/groupby.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/namespace.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/spark/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/spark/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/spark/utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/spark/accessors.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/spark/functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/indexing.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/pandas/mlflow.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/join.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/python/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/python/pyspark/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/python/pyspark/shell.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/storagelevel.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/tests/test_errors.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/exceptions/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/exceptions/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/exceptions/captured.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/exceptions/connect.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/exceptions/base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/errors/error_classes.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/profiler.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/worker.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/conf.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/pipeline.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tuning.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/clustering.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/param/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/param/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/param/shared.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/param/_shared_params_code_gen.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/stat.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/regression.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_tuning.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_persistence.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_util.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_stat.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_algorithms.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_image.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_model_cache.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_evaluation.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_param.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_feature.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_pipeline.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_training_summary.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_clustering.yaml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_regression.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_readable.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_classification.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_feature.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_param.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/typing/test_evaluation.yml\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_linalg.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tests/test_wrapper.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/recommendation.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/feature.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/linalg/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/linalg/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/common.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/_typing.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/wrapper.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/tree.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/image.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/model_cache.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/evaluation.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/distributor.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/torch_run_process_wrapper.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/tests/test_distributor.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/tests/test_log_communication.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/torch/log_communication.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/classification.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/functions.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/base.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/util.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/ml/fpm.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/instrumentation_utils.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/util.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/java_gateway.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/accumulators.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/clustering.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/stat/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/stat/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/stat/test.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/stat/KernelDensity.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/stat/_statistics.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/stat/distribution.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/regression.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/test_util.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/test_stat.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/test_algorithms.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/test_feature.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/test_streaming_algorithms.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tests/test_linalg.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/recommendation.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/feature.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/linalg/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/linalg/__init__.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/linalg/distributed.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/common.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/_typing.pyi\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/tree.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/evaluation.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/classification.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/util.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/fpm.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/mllib/random.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/shell.py\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark/rddsampler.py\n",
            "spark-3.4.0-bin-hadoop3/python/.coveragerc\n",
            "spark-3.4.0-bin-hadoop3/python/MANIFEST.in\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark.egg-info/\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark.egg-info/PKG-INFO\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark.egg-info/SOURCES.txt\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark.egg-info/top_level.txt\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark.egg-info/requires.txt\n",
            "spark-3.4.0-bin-hadoop3/python/pyspark.egg-info/dependency_links.txt\n",
            "spark-3.4.0-bin-hadoop3/python/run-tests.py\n",
            "spark-3.4.0-bin-hadoop3/python/run-tests\n",
            "spark-3.4.0-bin-hadoop3/python/lib/\n",
            "spark-3.4.0-bin-hadoop3/python/lib/py4j-0.10.9.7-src.zip\n",
            "spark-3.4.0-bin-hadoop3/python/lib/PY4J_LICENSE.txt\n",
            "spark-3.4.0-bin-hadoop3/python/lib/pyspark.zip\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/people_array_utf16le.json\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/streaming/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/streaming/text-test.txt\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/people1.json\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/people_array.json\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/people.json\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/text-test.txt\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=1/c=1/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/.part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc.crc\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/b=0/c=0/part-r-00000-829af031-b970-49d6-ad39-30460a0be2c8.orc\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/orc_partitioned/_SUCCESS\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/sql/ages_newlines.csv\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/userlib-0.1.zip\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/hello/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/hello/sub_hello/\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/hello/sub_hello/sub_hello.txt\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/hello/hello.txt\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/test_pytorch_training_file.py\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/SimpleHTTPServer.py\n",
            "spark-3.4.0-bin-hadoop3/python/test_support/userlibrary.py\n",
            "spark-3.4.0-bin-hadoop3/python/mypy.ini\n",
            "spark-3.4.0-bin-hadoop3/python/docs/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/Makefile\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_static/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_static/css/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_static/css/pyspark.css\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_static/copybutton.js\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/query_management.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/io.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/core_classes.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.ss/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.mllib.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.ml.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/groupby.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/resampling.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/extensions.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/series.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/window.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/io.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/indexing.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/general_functions.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/frame.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.pandas/ml.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/spark_session.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/protobuf.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/row.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/avro.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/configuration.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/window.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/grouping.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/observation.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/functions.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/udf.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/io.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/column.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/catalog.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/core_classes.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/data_types.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.sql/dataframe.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.streaming.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.resource.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/reference/pyspark.errors.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/getting_started/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/getting_started/quickstart_connect.ipynb\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/getting_started/install.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/getting_started/quickstart_df.ipynb\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/getting_started/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/getting_started/quickstart_ps.ipynb\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/sql/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/sql/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/sql/arrow_pandas.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/best_practices.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/from_to_dbms.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/faq.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/typehints.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/options.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/types.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/transform_apply.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/pandas_pyspark.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/pandas_on_spark/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/python_packaging.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/user_guide/arrow_pandas.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_templates/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_templates/autosummary/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_templates/autosummary/class_with_docs.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/_templates/autosummary/class.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/conf.py\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/migration_guide/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/migration_guide/koalas_to_pyspark.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/migration_guide/pyspark_upgrade.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/migration_guide/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/development/\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/development/debugging.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/development/contributing.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/development/setting_ide.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/development/index.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/source/development/testing.rst\n",
            "spark-3.4.0-bin-hadoop3/python/docs/make.bat\n",
            "spark-3.4.0-bin-hadoop3/python/docs/make2.bat\n",
            "spark-3.4.0-bin-hadoop3/python/setup.py\n",
            "spark-3.4.0-bin-hadoop3/python/run-tests-with-coverage\n",
            "spark-3.4.0-bin-hadoop3/python/test_coverage/\n",
            "spark-3.4.0-bin-hadoop3/python/test_coverage/conf/\n",
            "spark-3.4.0-bin-hadoop3/python/test_coverage/conf/spark-defaults.conf\n",
            "spark-3.4.0-bin-hadoop3/python/test_coverage/sitecustomize.py\n",
            "spark-3.4.0-bin-hadoop3/python/test_coverage/coverage_daemon.py\n",
            "spark-3.4.0-bin-hadoop3/python/dist/\n",
            "spark-3.4.0-bin-hadoop3/python/README.md\n",
            "spark-3.4.0-bin-hadoop3/licenses/\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-istack-commons-runtime.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-machinist.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-AnchorJS.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jaxb-runtime.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-arpack.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-pyrolite.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-leveldbjni.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-zstd.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jsp-api.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-modernizr.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jakarta.activation-api.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jakarta.xml.bind-api.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-vis-timeline.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-xmlenc.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-reflectasm.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-pmml-model.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jodd.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-bootstrap.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-mustache.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-slf4j.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-JTransforms.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-javassist.html\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-f2j.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-paranamer.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-d3.min.js.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jakarta-ws-rs-api\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-matchMedia-polyfill.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-join.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jline.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-blas.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-minlog.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-zstd-jni.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-spire.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-janino.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-sorttable.js.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jquery.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-cloudpickle.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-kryo.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-javolution.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-datatables.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-graphlib-dot.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-dagre-d3.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-re2j.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-JLargeArrays.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-jakarta-annotation-api\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-sbt-launch-lib.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-antlr.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-protobuf.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-json-formatter.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-dnsjava.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-javax-transaction-transaction-api.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-CC0.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-respond.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-scopt.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-py4j.txt\n",
            "spark-3.4.0-bin-hadoop3/licenses/LICENSE-automaton.txt\n",
            "spark-3.4.0-bin-hadoop3/RELEASE\n",
            "spark-3.4.0-bin-hadoop3/sbin/\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-history-server.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-thriftserver.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/spark-daemons.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-thriftserver.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-mesos-dispatcher.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/decommission-slave.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-workers.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-mesos-shuffle-service.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/decommission-worker.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-slave.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-slave.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-workers.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-mesos-shuffle-service.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-connect-server.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-master.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/slaves.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-mesos-dispatcher.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-worker.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-slaves.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/workers.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-all.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/spark-config.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-master.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/spark-daemon.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-worker.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-all.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-slaves.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/start-history-server.sh\n",
            "spark-3.4.0-bin-hadoop3/sbin/stop-connect-server.sh\n",
            "spark-3.4.0-bin-hadoop3/README.md\n",
            "spark-3.4.0-bin-hadoop3/bin/\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-sql2.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/sparkR\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-class2.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/pyspark\n",
            "spark-3.4.0-bin-hadoop3/bin/run-example\n",
            "spark-3.4.0-bin-hadoop3/bin/sparkR2.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/docker-image-tool.sh\n",
            "spark-3.4.0-bin-hadoop3/bin/pyspark.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-class\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-submit\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-sql\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-shell\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-sql.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/find-spark-home\n",
            "spark-3.4.0-bin-hadoop3/bin/run-example.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/find-spark-home.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/load-spark-env.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/beeline.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-class.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-shell.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/load-spark-env.sh\n",
            "spark-3.4.0-bin-hadoop3/bin/beeline\n",
            "spark-3.4.0-bin-hadoop3/bin/sparkR.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-connect-shell\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-shell2.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/pyspark2.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-submit.cmd\n",
            "spark-3.4.0-bin-hadoop3/bin/spark-submit2.cmd\n",
            "mv: cannot move 'spark-3.4.0-bin-hadoop3' to a subdirectory of itself, '/content/spark-3.4.0-bin-hadoop3/spark-3.4.0-bin-hadoop3'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installer findspark si ce n'est pas déjà fait\n",
        "!pip install findspark\n",
        "\n",
        "import os\n",
        "\n",
        "# Définir la variable d'environnement SPARK_HOME\n",
        "os.environ['SPARK_HOME'] = '/content/spark-3.4.0-bin-hadoop3'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpG6G2AL4NZt",
        "outputId": "9e1e9fe1-c463-4555-93af-8fe3daa3afc2"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: findspark in /usr/local/lib/python3.10/dist-packages (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Créer une session Spark\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"S3 Integration\") \\\n",
        "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:3.3.1\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.access.key\", 'AKIAWZYVJD7JSRUFW5MR') \\\n",
        "    .config(\"spark.hadoop.fs.s3a.secret.key\", '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') \\\n",
        "    .config(\"spark.hadoop.fs.s3a.endpoint\", \"s3.amazonaws.com\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
        "    .config(\"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.DefaultAWSCredentialsProviderChain\") \\\n",
        "    .getOrCreate()"
      ],
      "metadata": {
        "id": "fpkXb474g1xp"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sc = spark.sparkContext\n",
        "sc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "l8UsOgGe1db2",
        "outputId": "d4a6fb6f-7bf1-4486-a71d-58ad8fad6c73"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SparkContext master=local[*] appName=S3 Integration>"
            ],
            "text/html": [
              "\n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://cae5ceb96305:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.4.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>S3 Integration</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
        "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
      ],
      "metadata": {
        "id": "BYznLHqr8079"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo $AWS_SECRET_ACCESS_KEY\n",
        "!echo $AWS_ACCESS_KEY_ID"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ef78X3edlc1L",
        "outputId": "6b4ed7aa-a92c-45d8-f2cd-5b0f2d4f57c4"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Charger les fichiers d'images dans le dossier \"Test\"\n",
        "images = spark.read.format(\"binaryFile\") \\\n",
        "    .option(\"pathGlobFilter\", \"*.jpg\") \\\n",
        "    .option(\"recursiveFileLookup\", \"true\") \\\n",
        "    .load(\"s3a://oc-calculddist/Projet_agritech/data/Test/*/*\")\n",
        "\n",
        "images.show()"
      ],
      "metadata": {
        "id": "MHWaIgLF4D8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import boto3\n",
        "\n",
        "s3 = boto3.client(\n",
        "    's3',\n",
        "    aws_access_key_id='AKIAWZYVJD7JSRUFW5MR',\n",
        "    aws_secret_access_key='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',\n",
        ")"
      ],
      "metadata": {
        "id": "ZQQ0YPVxhq-z"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aws configure\n",
        "!pip install awscli\n",
        "!aws configure"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Jm5TWoQYjFU8",
        "outputId": "346e25c0-fe22-4edb-9408-e3d5861826b9"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting aws\n",
            "  Downloading aws-0.2.5.tar.gz (5.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting configure\n",
            "  Downloading configure-0.5.tar.gz (6.2 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting boto (from aws)\n",
            "  Downloading boto-2.49.0-py2.py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fabric>=1.6 (from aws)\n",
            "  Downloading fabric-3.2.2-py3-none-any.whl (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: prettytable>=0.7 in /usr/local/lib/python3.10/dist-packages (from aws) (3.10.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from configure) (6.0.1)\n",
            "Collecting invoke>=2.0 (from fabric>=1.6->aws)\n",
            "  Downloading invoke-2.2.0-py3-none-any.whl (160 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.3/160.3 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paramiko>=2.4 (from fabric>=1.6->aws)\n",
            "  Downloading paramiko-3.4.0-py3-none-any.whl (225 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m225.9/225.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting decorator>=5 (from fabric>=1.6->aws)\n",
            "  Downloading decorator-5.1.1-py3-none-any.whl (9.1 kB)\n",
            "Collecting deprecated>=1.2 (from fabric>=1.6->aws)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prettytable>=0.7->aws) (0.2.13)\n",
            "Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2->fabric>=1.6->aws) (1.14.1)\n",
            "Collecting bcrypt>=3.2 (from paramiko>=2.4->fabric>=1.6->aws)\n",
            "  Downloading bcrypt-4.1.3-cp39-abi3-manylinux_2_28_x86_64.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.7/283.7 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko>=2.4->fabric>=1.6->aws) (42.0.7)\n",
            "Collecting pynacl>=1.5 (from paramiko>=2.4->fabric>=1.6->aws)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko>=2.4->fabric>=1.6->aws) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko>=2.4->fabric>=1.6->aws) (2.22)\n",
            "Building wheels for collected packages: aws, configure\n",
            "  Building wheel for aws (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for aws: filename=aws-0.2.5-py3-none-any.whl size=7433 sha256=04a868b094acea575435313d172d8a75694c49efba3fdef685f29a52956827f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a0/d0/ac/e8d76c21487489b51cdef392e7d23a4ccb35f82633d164ec0c\n",
            "  Building wheel for configure (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configure: filename=configure-0.5-py3-none-any.whl size=6473 sha256=d7d84daed259434d3684988d0634ba3ecea0a0418f1fc119246041e0ae1dcf67\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/32/6a/0e19b0b63fa25686cac71da1717ca61041e18bb826ae62bb36\n",
            "Successfully built aws configure\n",
            "Installing collected packages: boto, invoke, deprecated, decorator, configure, bcrypt, pynacl, paramiko, fabric, aws\n",
            "  Attempting uninstall: decorator\n",
            "    Found existing installation: decorator 4.4.2\n",
            "    Uninstalling decorator-4.4.2:\n",
            "      Successfully uninstalled decorator-4.4.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed aws-0.2.5 bcrypt-4.1.3 boto-2.49.0 configure-0.5 decorator-5.1.1 deprecated-1.2.14 fabric-3.2.2 invoke-2.2.0 paramiko-3.4.0 pynacl-1.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "decorator"
                ]
              },
              "id": "249a271d2dfe4b92a3458750cad0fcd9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting awscli\n",
            "  Downloading awscli-1.33.4-py3-none-any.whl (4.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: botocore==1.34.122 in /usr/local/lib/python3.10/dist-packages (from awscli) (1.34.122)\n",
            "Collecting docutils<0.17,>=0.10 (from awscli)\n",
            "  Downloading docutils-0.16-py2.py3-none-any.whl (548 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m548.2/548.2 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.11.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from awscli) (0.10.1)\n",
            "Requirement already satisfied: PyYAML<6.1,>=3.10 in /usr/local/lib/python3.10/dist-packages (from awscli) (6.0.1)\n",
            "Collecting colorama<0.4.7,>=0.2.5 (from awscli)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting rsa<4.8,>=3.1.2 (from awscli)\n",
            "  Downloading rsa-4.7.2-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.122->awscli) (1.0.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.122->awscli) (2.8.2)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore==1.34.122->awscli) (2.0.7)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.10/dist-packages (from rsa<4.8,>=3.1.2->awscli) (0.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.34.122->awscli) (1.16.0)\n",
            "Installing collected packages: rsa, docutils, colorama, awscli\n",
            "  Attempting uninstall: rsa\n",
            "    Found existing installation: rsa 4.9\n",
            "    Uninstalling rsa-4.9:\n",
            "      Successfully uninstalled rsa-4.9\n",
            "  Attempting uninstall: docutils\n",
            "    Found existing installation: docutils 0.18.1\n",
            "    Uninstalling docutils-0.18.1:\n",
            "      Successfully uninstalled docutils-0.18.1\n",
            "Successfully installed awscli-1.33.4 colorama-0.4.6 docutils-0.16 rsa-4.7.2\n",
            "AWS Access Key ID [None]: AKIAWZYVJD7JSRUFW5MR\n",
            "AWS Secret Access Key [None]: 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\n",
            "Default region name [None]: eu-west-3\n",
            "Default output format [None]: json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "io52heVCm93-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "SW4Qvg9egZQq"
      },
      "outputs": [],
      "source": [
        "# Détails code : A2\n",
        "model = MobileNetV2(weights='imagenet',\n",
        "                    include_top=True,\n",
        "                    input_shape=(224, 224, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "obGcAskEgZjI"
      },
      "outputs": [],
      "source": [
        "# Voir détails - code : A3\n",
        "new_model = Model(inputs=model.input,\n",
        "                  outputs=model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "UV4EZTmggZmC"
      },
      "outputs": [],
      "source": [
        "brodcast_weights = sc.broadcast(new_model.get_weights())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLcBL_uMgbWd",
        "outputId": "cb7cc446-c952-4140-86f6-9188fb917f67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                Output Shape                 Param #   Connected to                  \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)        [(None, 224, 224, 3)]        0         []                            \n",
            "                                                                                                  \n",
            " Conv1 (Conv2D)              (None, 112, 112, 32)         864       ['input_3[0][0]']             \n",
            "                                                                                                  \n",
            " bn_Conv1 (BatchNormalizati  (None, 112, 112, 32)         128       ['Conv1[0][0]']               \n",
            " on)                                                                                              \n",
            "                                                                                                  \n",
            " Conv1_relu (ReLU)           (None, 112, 112, 32)         0         ['bn_Conv1[0][0]']            \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise (D  (None, 112, 112, 32)         288       ['Conv1_relu[0][0]']          \n",
            " epthwiseConv2D)                                                                                  \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_BN  (None, 112, 112, 32)         128       ['expanded_conv_depthwise[0][0\n",
            "  (BatchNormalization)                                              ]']                           \n",
            "                                                                                                  \n",
            " expanded_conv_depthwise_re  (None, 112, 112, 32)         0         ['expanded_conv_depthwise_BN[0\n",
            " lu (ReLU)                                                          ][0]']                        \n",
            "                                                                                                  \n",
            " expanded_conv_project (Con  (None, 112, 112, 16)         512       ['expanded_conv_depthwise_relu\n",
            " v2D)                                                               [0][0]']                      \n",
            "                                                                                                  \n",
            " expanded_conv_project_BN (  (None, 112, 112, 16)         64        ['expanded_conv_project[0][0]'\n",
            " BatchNormalization)                                                ]                             \n",
            "                                                                                                  \n",
            " block_1_expand (Conv2D)     (None, 112, 112, 96)         1536      ['expanded_conv_project_BN[0][\n",
            "                                                                    0]']                          \n",
            "                                                                                                  \n",
            " block_1_expand_BN (BatchNo  (None, 112, 112, 96)         384       ['block_1_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_1_expand_relu (ReLU)  (None, 112, 112, 96)         0         ['block_1_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_1_pad (ZeroPadding2D  (None, 113, 113, 96)         0         ['block_1_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_1_depthwise (Depthwi  (None, 56, 56, 96)           864       ['block_1_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_1_depthwise_BN (Batc  (None, 56, 56, 96)           384       ['block_1_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_1_depthwise_relu (Re  (None, 56, 56, 96)           0         ['block_1_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_1_project (Conv2D)    (None, 56, 56, 24)           2304      ['block_1_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_1_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_1_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_expand (Conv2D)     (None, 56, 56, 144)          3456      ['block_1_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_2_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_2_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_2_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_2_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_2_depthwise (Depthwi  (None, 56, 56, 144)          1296      ['block_2_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_2_depthwise_BN (Batc  (None, 56, 56, 144)          576       ['block_2_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_2_depthwise_relu (Re  (None, 56, 56, 144)          0         ['block_2_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_2_project (Conv2D)    (None, 56, 56, 24)           3456      ['block_2_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_2_project_BN (BatchN  (None, 56, 56, 24)           96        ['block_2_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_2_add (Add)           (None, 56, 56, 24)           0         ['block_1_project_BN[0][0]',  \n",
            "                                                                     'block_2_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_3_expand (Conv2D)     (None, 56, 56, 144)          3456      ['block_2_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_3_expand_BN (BatchNo  (None, 56, 56, 144)          576       ['block_3_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_3_expand_relu (ReLU)  (None, 56, 56, 144)          0         ['block_3_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_3_pad (ZeroPadding2D  (None, 57, 57, 144)          0         ['block_3_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_3_depthwise (Depthwi  (None, 28, 28, 144)          1296      ['block_3_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_3_depthwise_BN (Batc  (None, 28, 28, 144)          576       ['block_3_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_3_depthwise_relu (Re  (None, 28, 28, 144)          0         ['block_3_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_3_project (Conv2D)    (None, 28, 28, 32)           4608      ['block_3_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_3_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_3_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_3_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_4_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_4_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_4_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_4_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_4_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_4_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_4_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_4_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_4_depthwise_relu (Re  (None, 28, 28, 192)          0         ['block_4_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_4_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_4_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_4_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_4_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_4_add (Add)           (None, 28, 28, 32)           0         ['block_3_project_BN[0][0]',  \n",
            "                                                                     'block_4_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_5_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_4_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_5_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_5_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_5_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_5_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_5_depthwise (Depthwi  (None, 28, 28, 192)          1728      ['block_5_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_5_depthwise_BN (Batc  (None, 28, 28, 192)          768       ['block_5_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_5_depthwise_relu (Re  (None, 28, 28, 192)          0         ['block_5_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_5_project (Conv2D)    (None, 28, 28, 32)           6144      ['block_5_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_5_project_BN (BatchN  (None, 28, 28, 32)           128       ['block_5_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_5_add (Add)           (None, 28, 28, 32)           0         ['block_4_add[0][0]',         \n",
            "                                                                     'block_5_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_6_expand (Conv2D)     (None, 28, 28, 192)          6144      ['block_5_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_6_expand_BN (BatchNo  (None, 28, 28, 192)          768       ['block_6_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_6_expand_relu (ReLU)  (None, 28, 28, 192)          0         ['block_6_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_6_pad (ZeroPadding2D  (None, 29, 29, 192)          0         ['block_6_expand_relu[0][0]'] \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_6_depthwise (Depthwi  (None, 14, 14, 192)          1728      ['block_6_pad[0][0]']         \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_6_depthwise_BN (Batc  (None, 14, 14, 192)          768       ['block_6_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_6_depthwise_relu (Re  (None, 14, 14, 192)          0         ['block_6_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_6_project (Conv2D)    (None, 14, 14, 64)           12288     ['block_6_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_6_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_6_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_7_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_6_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_7_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_7_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_7_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_7_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_7_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_7_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_7_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_7_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_7_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_7_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_7_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_7_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_7_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_7_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_7_add (Add)           (None, 14, 14, 64)           0         ['block_6_project_BN[0][0]',  \n",
            "                                                                     'block_7_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_8_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_7_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_8_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_8_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_8_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_8_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_8_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_8_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_8_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_8_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_8_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_8_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_8_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_8_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_8_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_8_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_8_add (Add)           (None, 14, 14, 64)           0         ['block_7_add[0][0]',         \n",
            "                                                                     'block_8_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_9_expand (Conv2D)     (None, 14, 14, 384)          24576     ['block_8_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_9_expand_BN (BatchNo  (None, 14, 14, 384)          1536      ['block_9_expand[0][0]']      \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " block_9_expand_relu (ReLU)  (None, 14, 14, 384)          0         ['block_9_expand_BN[0][0]']   \n",
            "                                                                                                  \n",
            " block_9_depthwise (Depthwi  (None, 14, 14, 384)          3456      ['block_9_expand_relu[0][0]'] \n",
            " seConv2D)                                                                                        \n",
            "                                                                                                  \n",
            " block_9_depthwise_BN (Batc  (None, 14, 14, 384)          1536      ['block_9_depthwise[0][0]']   \n",
            " hNormalization)                                                                                  \n",
            "                                                                                                  \n",
            " block_9_depthwise_relu (Re  (None, 14, 14, 384)          0         ['block_9_depthwise_BN[0][0]']\n",
            " LU)                                                                                              \n",
            "                                                                                                  \n",
            " block_9_project (Conv2D)    (None, 14, 14, 64)           24576     ['block_9_depthwise_relu[0][0]\n",
            "                                                                    ']                            \n",
            "                                                                                                  \n",
            " block_9_project_BN (BatchN  (None, 14, 14, 64)           256       ['block_9_project[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_9_add (Add)           (None, 14, 14, 64)           0         ['block_8_add[0][0]',         \n",
            "                                                                     'block_9_project_BN[0][0]']  \n",
            "                                                                                                  \n",
            " block_10_expand (Conv2D)    (None, 14, 14, 384)          24576     ['block_9_add[0][0]']         \n",
            "                                                                                                  \n",
            " block_10_expand_BN (BatchN  (None, 14, 14, 384)          1536      ['block_10_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_10_expand_relu (ReLU  (None, 14, 14, 384)          0         ['block_10_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_10_depthwise (Depthw  (None, 14, 14, 384)          3456      ['block_10_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_10_depthwise_BN (Bat  (None, 14, 14, 384)          1536      ['block_10_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_10_depthwise_relu (R  (None, 14, 14, 384)          0         ['block_10_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_10_project (Conv2D)   (None, 14, 14, 96)           36864     ['block_10_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_10_project_BN (Batch  (None, 14, 14, 96)           384       ['block_10_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_11_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_10_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_11_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_11_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_11_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_11_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_11_depthwise (Depthw  (None, 14, 14, 576)          5184      ['block_11_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_11_depthwise_BN (Bat  (None, 14, 14, 576)          2304      ['block_11_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_11_depthwise_relu (R  (None, 14, 14, 576)          0         ['block_11_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_11_project (Conv2D)   (None, 14, 14, 96)           55296     ['block_11_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_11_project_BN (Batch  (None, 14, 14, 96)           384       ['block_11_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_11_add (Add)          (None, 14, 14, 96)           0         ['block_10_project_BN[0][0]', \n",
            "                                                                     'block_11_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_12_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_11_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_12_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_12_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_12_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_12_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_12_depthwise (Depthw  (None, 14, 14, 576)          5184      ['block_12_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_12_depthwise_BN (Bat  (None, 14, 14, 576)          2304      ['block_12_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_12_depthwise_relu (R  (None, 14, 14, 576)          0         ['block_12_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_12_project (Conv2D)   (None, 14, 14, 96)           55296     ['block_12_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_12_project_BN (Batch  (None, 14, 14, 96)           384       ['block_12_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_12_add (Add)          (None, 14, 14, 96)           0         ['block_11_add[0][0]',        \n",
            "                                                                     'block_12_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_13_expand (Conv2D)    (None, 14, 14, 576)          55296     ['block_12_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_13_expand_BN (BatchN  (None, 14, 14, 576)          2304      ['block_13_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_13_expand_relu (ReLU  (None, 14, 14, 576)          0         ['block_13_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_13_pad (ZeroPadding2  (None, 15, 15, 576)          0         ['block_13_expand_relu[0][0]']\n",
            " D)                                                                                               \n",
            "                                                                                                  \n",
            " block_13_depthwise (Depthw  (None, 7, 7, 576)            5184      ['block_13_pad[0][0]']        \n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_13_depthwise_BN (Bat  (None, 7, 7, 576)            2304      ['block_13_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_13_depthwise_relu (R  (None, 7, 7, 576)            0         ['block_13_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_13_project (Conv2D)   (None, 7, 7, 160)            92160     ['block_13_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_13_project_BN (Batch  (None, 7, 7, 160)            640       ['block_13_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_14_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_13_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_14_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_14_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_14_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_14_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_14_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_14_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_14_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_14_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_14_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_14_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_14_project (Conv2D)   (None, 7, 7, 160)            153600    ['block_14_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_14_project_BN (Batch  (None, 7, 7, 160)            640       ['block_14_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_14_add (Add)          (None, 7, 7, 160)            0         ['block_13_project_BN[0][0]', \n",
            "                                                                     'block_14_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_15_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_14_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_15_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_15_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_15_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_15_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_15_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_15_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_15_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_15_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_15_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_15_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_15_project (Conv2D)   (None, 7, 7, 160)            153600    ['block_15_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_15_project_BN (Batch  (None, 7, 7, 160)            640       ['block_15_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " block_15_add (Add)          (None, 7, 7, 160)            0         ['block_14_add[0][0]',        \n",
            "                                                                     'block_15_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " block_16_expand (Conv2D)    (None, 7, 7, 960)            153600    ['block_15_add[0][0]']        \n",
            "                                                                                                  \n",
            " block_16_expand_BN (BatchN  (None, 7, 7, 960)            3840      ['block_16_expand[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " block_16_expand_relu (ReLU  (None, 7, 7, 960)            0         ['block_16_expand_BN[0][0]']  \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " block_16_depthwise (Depthw  (None, 7, 7, 960)            8640      ['block_16_expand_relu[0][0]']\n",
            " iseConv2D)                                                                                       \n",
            "                                                                                                  \n",
            " block_16_depthwise_BN (Bat  (None, 7, 7, 960)            3840      ['block_16_depthwise[0][0]']  \n",
            " chNormalization)                                                                                 \n",
            "                                                                                                  \n",
            " block_16_depthwise_relu (R  (None, 7, 7, 960)            0         ['block_16_depthwise_BN[0][0]'\n",
            " eLU)                                                               ]                             \n",
            "                                                                                                  \n",
            " block_16_project (Conv2D)   (None, 7, 7, 320)            307200    ['block_16_depthwise_relu[0][0\n",
            "                                                                    ]']                           \n",
            "                                                                                                  \n",
            " block_16_project_BN (Batch  (None, 7, 7, 320)            1280      ['block_16_project[0][0]']    \n",
            " Normalization)                                                                                   \n",
            "                                                                                                  \n",
            " Conv_1 (Conv2D)             (None, 7, 7, 1280)           409600    ['block_16_project_BN[0][0]'] \n",
            "                                                                                                  \n",
            " Conv_1_bn (BatchNormalizat  (None, 7, 7, 1280)           5120      ['Conv_1[0][0]']              \n",
            " ion)                                                                                             \n",
            "                                                                                                  \n",
            " out_relu (ReLU)             (None, 7, 7, 1280)           0         ['Conv_1_bn[0][0]']           \n",
            "                                                                                                  \n",
            " global_average_pooling2d_2  (None, 1280)                 0         ['out_relu[0][0]']            \n",
            "  (GlobalAveragePooling2D)                                                                        \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2257984 (8.61 MB)\n",
            "Trainable params: 2223872 (8.48 MB)\n",
            "Non-trainable params: 34112 (133.25 KB)\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "new_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "-pYML_Ah2X06"
      },
      "outputs": [],
      "source": [
        "def model_fn():\n",
        "    model = MobileNetV2(weights='imagenet',\n",
        "                        include_top=True,\n",
        "                        input_shape=(224, 224, 3))\n",
        "    for layer in model.layers:\n",
        "        layer.trainable = False\n",
        "    new_model = Model(inputs=model.input,\n",
        "                  outputs=model.layers[-2].output)\n",
        "    new_model.set_weights(brodcast_weights.value)\n",
        "    return new_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDbtHVnZ2YvX",
        "outputId": "41ab88e9-52d0-4553-dc81-e8b287e28e64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/pyspark/sql/pandas/functions.py:407: UserWarning: In Python 3.6+ and Spark 3.0+, it is preferred to specify type hints for pandas UDF instead of specifying pandas UDF type which will be deprecated in the future releases. See SPARK-28264 for more details.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "def preprocess(content):\n",
        "    img = Image.open(io.BytesIO(content)).resize([224, 224])\n",
        "    arr = img_to_array(img)\n",
        "    return preprocess_input(arr)\n",
        "\n",
        "def featurize_series(model, content_series):\n",
        "    input = np.stack(content_series.map(preprocess))\n",
        "    preds = model.predict(input)\n",
        "    output = [p.flatten() for p in preds]\n",
        "    return pd.Series(output)\n",
        "\n",
        "@pandas_udf('array<float>', PandasUDFType.SCALAR_ITER)\n",
        "def featurize_udf(content_series_iter):\n",
        "    model = model_fn()\n",
        "    for content_series in content_series_iter:\n",
        "        yield featurize_series(model, content_series)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Qt5sU9yG2Y1x"
      },
      "outputs": [],
      "source": [
        "spark.conf.set(\"spark.sql.execution.arrow.maxRecordsPerBatch\", \"1024\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N72oz5AjgbZv",
        "outputId": "a9729f88-e212-4db0-bcb5-3dbb266613ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- path: string (nullable = true)\n",
            " |-- modificationTime: timestamp (nullable = true)\n",
            " |-- length: long (nullable = true)\n",
            " |-- content: binary (nullable = true)\n",
            " |-- label: string (nullable = true)\n",
            "\n",
            "+--------------------+-----+--------------------+\n",
            "|                path|label|            features|\n",
            "+--------------------+-----+--------------------+\n",
            "|file:/content/dri...|     |[0.33079213, 0.03...|\n",
            "|file:/content/dri...|     |[0.7513627, 1.620...|\n",
            "|file:/content/dri...|     |[0.016479073, 0.4...|\n",
            "|file:/content/dri...|     |[0.067617506, 0.3...|\n",
            "|file:/content/dri...|     |[0.0, 0.85379255,...|\n",
            "|file:/content/dri...|     |[0.34845024, 0.02...|\n",
            "|file:/content/dri...|     |[1.541634, 0.3627...|\n",
            "|file:/content/dri...|     |[0.50183904, 0.23...|\n",
            "|file:/content/dri...|     |[0.0, 1.1315749, ...|\n",
            "|file:/content/dri...|     |[0.20974927, 0.33...|\n",
            "|file:/content/dri...|     |[0.23668702, 0.19...|\n",
            "|file:/content/dri...|     |[1.2100002, 0.003...|\n",
            "|file:/content/dri...|     |[1.0812459, 0.081...|\n",
            "|file:/content/dri...|     |[0.89299774, 0.00...|\n",
            "|file:/content/dri...|     |[0.88253194, 0.58...|\n",
            "|file:/content/dri...|     |[0.9348819, 0.796...|\n",
            "|file:/content/dri...|     |[0.16921404, 0.11...|\n",
            "|file:/content/dri...|     |[0.06056695, 0.0,...|\n",
            "|file:/content/dri...|     |[0.0029379593, 0....|\n",
            "|file:/content/dri...|     |[0.86853975, 0.0,...|\n",
            "+--------------------+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import regexp_extract\n",
        "\n",
        "images = images.withColumn(\"label\", regexp_extract(\"path\", \"([^/]+)_image\", 1))\n",
        "images.printSchema()\n",
        "features_df = images.repartition(24).select(\n",
        "    col(\"path\"),\n",
        "    col(\"label\"),\n",
        "    featurize_udf(\"content\").alias(\"features\")\n",
        ")\n",
        "features_df.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "dtmiUS0_3oTO"
      },
      "outputs": [],
      "source": [
        "from pyspark.ml.feature import StandardScaler, PCA\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_pca(dataframe):\n",
        "\n",
        "    # Préparation des données - conversion des données images en vecteur dense\n",
        "    transform_vecteur_dense = udf(lambda r: Vectors.dense(r), VectorUDT())\n",
        "    dataframe = dataframe.withColumn('features_vectors', transform_vecteur_dense('features'))\n",
        "\n",
        "    # Standardisation obligatoire pour PCA\n",
        "    scaler_std = StandardScaler(inputCol=\"features_vectors\", outputCol=\"features_scaled\", withStd=True, withMean=True)\n",
        "    model_std = scaler_std.fit(dataframe)\n",
        "    # Mise à l'échelle\n",
        "    dataframe = model_std.transform(dataframe)\n",
        "\n",
        "    return dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "nFgK8kYO3oYa"
      },
      "outputs": [],
      "source": [
        "def recherche_nb_composante(dataframe, nb_comp=400):\n",
        "\n",
        "    pca = PCA(k=nb_comp, inputCol=\"features_scaled\", outputCol=\"features_pca\")\n",
        "    model_pca = pca.fit(dataframe)\n",
        "    variance = model_pca.explainedVariance\n",
        "\n",
        "    # visuel\n",
        "    import matplotlib.pyplot as plt\n",
        "    plt.plot(np.arange(len(variance)) + 1, variance.cumsum(), c=\"red\", marker='o')\n",
        "    plt.xlabel(\"Nb composantes\")\n",
        "    plt.ylabel(\"% variance\")\n",
        "    plt.show(block=False)\n",
        "\n",
        "    def nb_comp():\n",
        "        for i in range(len(variance)):\n",
        "            if variance.cumsum()[i] >= 0.95:\n",
        "                print(\"{} composantes principales expliquent au moins 95% de la variance totale\".format(i + 1))\n",
        "                return i + 1\n",
        "\n",
        "    k = nb_comp()\n",
        "    return k\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "yHFCckW13oey",
        "outputId": "7f079353-1e4c-4454-ac51-16bb75b23efe"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA9DElEQVR4nO3dd3jUVaL/8c8kkEJJAgSSAJGANLk0BYmIKF6yoq4KyyKIrCC6WEBEKS7YEBu6Kj/QRVHsXpUawb0iFiSK3AgKBBtGgmBCCS2SUKRNzu+P2YwMpMwkU7/zfj1PnmS+3zMz5zDRfJ5TbcYYIwAAAIuICHQFAAAAvIlwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALKVWoCvgb6Wlpdq5c6fq168vm80W6OoAAAA3GGN08OBBNW3aVBERlffNhF242blzp1JTUwNdDQAAUA0FBQVq3rx5pWXCLtzUr19fkuMfJy4uLsC1AQAA7igpKVFqaqrz73hlwi7clA1FxcXFEW4AAAgx7kwpYUIxAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwFMINAACwlLDboRgAANSA3S5lZUmffSZt2yYZI0VESKmpUsOG0oEDjsd9+ji+IiP9XkXCDQAA4chul1atknbskHbvlvbulfLzHWHlVKcGl+xs6YMPpOPHq379Rx+VGjWSXnpJGjjQN22oAOEGAAArqKpHpajoj/BSUCCtXy8dOeLbOu3fL/31r9LixX4NOIQbAACCVXmBpUx1e1QCYdw4qX9/vw1REW4AAPCXsrCSlSWVlkoJCa49KmUiIhy9Kh995PveFX/Yvt0xBNanj1/ejnADAEBNnN67YrNVPBS0dm3w9q742q5dfnsrwg0AAOVxZw7L6tXSmjXhG1g8kZLit7ci3AAAwk9Fc1nKwsuOHdLChdLRo4GspXU0by717u23tyPcAACsw50JuAQX/5s1y6/73RBuAACho7IeFytNwLUK9rkBAIQ1hopCT+3aUnq61KIFOxQDAMIQvS7BKzZWuvxyx/fKdiguKnIs6z7rLOm//ztg4aUqhBsAgHdUtrqIXhf/Kq9H5fT9dCIiHPeDOKRUF+EGAOCeyvZzCfYdcq0gJkbq3t3xby6FZI+KvxBuAAAOhBf/iomRrr1Watas/B2KU1OlxEQpOdlRpnfvsA4sniDcAEA4qWj7fzaj847ataULLpB69Qq7oaBgQrgBACsqb/5LuG//XxOVzWGx2QgsQYZwAwChqqLVR9u3S19/zeRdd5UFl9RU13ksQbCkGdVDuAGAYFbRCiSWTletsgm4BBdLI9wAQDAoby4Mk3grV16PCxNwIcINAPjf6b0x+fnMhTndqb0u9LjAQ4QbAPCV8pZWM5zkQK8LfIhwAwA1xZDSmYL0zCGEB8INAHiCISWH8vZzYYdcBAnCDQCUJ9yHlAgvCGGEGwCQXMPMqlXhsVvv6dv/sxkdLIJwAyC8lLdvjJU3vYuOlnr0kJo3dzxm+3+EAcINAOs6faJvQYG0cKE1Q8zpS6cJMAhjhBsA1mH1oaXTVyCxdBooF+EGQGiycq/MqXNhmMQLeIxwAyA0WLVXpmw4qUULhpIALyHcAAg+VuyVOXVpNcNJgE8RbgAEnpV6ZRhSAgKOcAPA/+x2R4jZsUP65BNp/vzQ7JVhSAkISoQbAL53es/MunWhtcsvQ0pASCHcAPC+UO6ZKdv07qyz6I0BQhThBkDNhWrPzKn7xhBkAMsg3AConrJA8/zz0v/+b/BPAC6b6Fu2g2+fPgQZwKIINwDcc2rvzJdfSmvXBvdQU9k8md696ZEBwgzhBkDFQqV3hl4ZAKcg3AD4Q6hMBKZXBkAlCDdAuDu1d2b58uCbCEyvDAAPEW6AcBTMw030ygCoIcINEA6CeTJwbKx0xRVShw70ygDwCsINYFXB2jtDzwwAHyPcAFZjt0uPPCI9+WRw9M7QMwPAzwg3QKg7fcgpO1s6cSJw9aFnBkCAEW6AUBRMQ07R0Y4jDC66iDADICgQboBQEixDTlFR0lVXSaNHE2YABB3CDRDMgmnIiUADIEQQboBgFAw9NEwEBhCiCDdAMCkLNY8/HpgeGnpnAFgA4QYIpEAPOzEZGIAFEW6AQAjksBO9MwAsjnAD+FOghp0INADCCOEG8DW7XVq1SlqyRHrhBf/tSRMT4wg0t91GoAEQVgg3gK+U9dLMmCEdPOif96SHBgAIN4BXnbpz8PvvSydP+v49CTQA4IJwA3iDvycIM+QEABUi3ADVVdZLM2eOtHSp7ycIR0ZK/fvTQwMAVSDcAJ7ydy9NbKx0zz3SAw8QaADADYQbwF3+XMbNsBMAVFtEoCswe/ZspaWlKSYmRunp6Vq7dm2l5WfOnKl27dopNjZWqampuvvuu3U0kKcjIzwsWiTVry9Nm+a7YBMZKQ0cKH36qXTokLRwodS3L8EGADwU0HAzf/58jR8/XlOnTtX69evVpUsX9evXT3v27Cm3/DvvvKPJkydr6tSp2rRpk1555RXNnz9f9957r59rjrBgt0srVkgXXihde630++++eZ/ataWpU6Vjx6TFiwk0AFBDNmOMCdSbp6en6/zzz9e//vUvSVJpaalSU1M1duxYTZ48+Yzyd9xxhzZt2qQVK1Y4r02YMEFr1qzRl19+6dZ7lpSUKD4+XsXFxYqLi/NOQ2At/ppTU7++NH48c2kAwA2e/P0OWM/N8ePHtW7dOmVkZPxRmYgIZWRkKDs7u9znXHjhhVq3bp1z6OqXX37RsmXLdOWVV1b4PseOHVNJSYnLF1Auu1166CHHBN5p03wTbKKipDvvlFaulH77zfF+BBsA8KqATSjet2+f7Ha7kpKSXK4nJSXpp59+Kvc5119/vfbt26eLLrpIxhidPHlSt912W6XDUtOnT9e0adO8WndYjD8mCrPiCQD8JuATij2RlZWlxx9/XM8//7zWr1+vzMxMffDBB3rkkUcqfM6UKVNUXFzs/CooKPBjjRHU7Hbp4YelevV8M1E4KuqPCcIHD9JLAwB+ErCem8TEREVGRmr37t0u13fv3q3k5ORyn/PAAw/ohhtu0N///ndJUqdOnXT48GHdcsstuu+++xQRcWZWi46OVnR0tPcbgNDlyzk1tWs7NtpjCTcABEzAem6ioqLUrVs3l8nBpaWlWrFihXr27Fnuc44cOXJGgIn8zx+PAM6LRqjw5ZyashVPv//OEm4ACLCAbuI3fvx4jRgxQt27d1ePHj00c+ZMHT58WCNHjpQkDR8+XM2aNdP06dMlSVdffbVmzJihc889V+np6crLy9MDDzygq6++2hlygHItWiQNH+795dy1a0v33stcGgAIIgENN0OGDNHevXv14IMPqrCwUF27dtXy5cudk4zz8/Ndemruv/9+2Ww23X///dqxY4caN26sq6++Wo899ligmoBgZ7dL118vLVjg3ddlgjAABK2A7nMTCOxzEwZOPdByyRLp5EnvvXZMjDRlinTffYQaAPAjT/5+c7YUrGXRIunmmyVv72fE8BMAhAzCDazBV8NPhBoACDmEG4S+RYukG27w7uon5tQAQMgi3CA02e3SqlXS009LH3zgvde94ALp0UfZowYAQhjhBqFn0SJp9Ghp717vvWZsrPTGG47TvwEAIS2kjl8ANGmSI4B4K9iUbb538CDBBgAsgp4bhAa7XbruOkevjbcMHiy98w7DTwBgMfTcIPgtWuQ43NJbwSYuzrGqav58gg0AWBDhBsGtbBiqpiuhateWBg1ynNBdVMQQFABYGMNSCD5lK6Geekpatqzmr8fwEwCEFcINgos3V0KxAgoAwhLDUgge3lwJNXgwK6AAIEwRbhAcJkxwbMhXU40bM1kYAMIcw1IILG8t8b7ySkfPT+/ehBoACHOEGwSOt86E8lavDwDAEhiWQmB4Y4l3bKxjCIpgAwA4BT038C+7XRo6VFq4sGavc+210rvvMgQFADgDPTfwn0WLpAYNah5sJkxw9NgQbAAA5SDcwD/KhqEOHqz+a5SthGIYCgBQCcINfK+mE36vvFJauVLatYt9awAAVWLODXzHG8u8x4+XnnnGe3UCAFgePTfwDW+c5D1hAsEGAOAxem7gXXa7NGyYY4fg6uJMKABADdBzA+8pWw1Vk2DDmVAAgBoi3MA7aroaKi6OM6EAAF7BsBRqbsIEacaM6j9/8GDpnXcINQAAr6DnBjUzcWLNgs348fTWAAC8inCD6lu4sGarmVjmDQDwAcINqsdul/7+9+o/n2XeAAAfYc4Nquf666WSEs+fxzJvAICP0XMDz5UdXOmJ2Fhp6lSWeQMAfI6eG7jPbpeGDvX8VG9WQwEA/IieG7inbIM+T4MNq6EAAH5GuEHVqrtB37XXMmkYAOB3DEuhchMnVi+g1K8vvfuu9+sDAEAV6LlBxWqyj82rrzIUBQAICMINyleTfWwmTpQGDfJufQAAcBPhBuWr7j4248dLTz3l/foAAOAmwg3ONHGi5/vYSOw6DAAICkwohqvqzLOJi5NefpnN+QAAQYFwgz9UZ54NG/QBAIIM4QYOdrv0pz95Ns/m2msdG/QBABBEmHMDKTNTatJEWrnS/eewjw0AIEjRcxPuMjOlv/7V8+exjw0AIEjRcxPO7Hbplls8fx772AAAghjhJpwNGybt3+/Zc669ln1sAABBjXATrhYu9HwyMPNsAAAhgHATjqp7tALzbAAAIYBwE46GDfP8aIVJk5hnAwAICayWCjeeDkfVqSO9/jq7DwMAQgbhJpx4OhxVp470229SVJTv6gQAgJcxLBVOPB2OeuMNgg0AIOQQbsKFp8NRQ4YwxwYAEJIIN+HA0+Go+vWlt9/2XX0AAPAhwk04ePRRz4ajWPINAAhhhBurW7hQmjbN/fIMRwEAQhyrpawsM1MaPNj98gxHAQAsgJ4bq6rOoZgMRwEALIBwY1WeHorJcBQAwCIIN1bk6bLvhg0ZjgIAWAbhxmqqcyjm3LkMRwEALINwYzWPPeb+su+ICEcvz8CBvq0TAAB+RLixErtdeuop98s/8ADzbAAAlkO4sZLHHpMOHXKvbFycI9wAAGAx1Qo3q1at0t/+9jf17NlTO3bskCS99dZb+vLLL71aOXjA016bV15hng0AwJI8DjeLFy9Wv379FBsbqw0bNujYsWOSpOLiYj3++ONeryDc5EmvDcu+AQAW5nG4efTRRzVnzhzNnTtXtWvXdl7v1auX1q9f79XKwU12uzRrlntl69Zl2TcAwNI8Dje5ubm6+OKLz7geHx+vAwcOeKNO8NSqVVJRkXtl77mH4SgAgKV5HG6Sk5OVl5d3xvUvv/xSrVq18kql4KElS9wrV6+edN99Pq0KAACB5nG4GTVqlMaNG6c1a9bIZrNp586devvttzVx4kTdfvvtHldg9uzZSktLU0xMjNLT07V27dpKyx84cEBjxoxRSkqKoqOj1bZtWy1btszj97WMhQul555zr+ykSfTaAAAsz+NTwSdPnqzS0lL17dtXR44c0cUXX6zo6GhNnDhRY8eO9ei15s+fr/Hjx2vOnDlKT0/XzJkz1a9fP+Xm5qpJkyZnlD9+/Lj+9Kc/qUmTJlq0aJGaNWumX3/9VQkJCZ42wxo8OfU7Lo5eGwBAWLAZY0x1nnj8+HHl5eXp0KFD6tChg+rVq+fxa6Snp+v888/Xv/71L0lSaWmpUlNTNXbsWE2ePPmM8nPmzNFTTz2ln376yWUysydKSkoUHx+v4uJixcXFVes1goLdLiUluX845l13Sf/v//m0SgAA+Ionf789HpYqLi5WUVGRoqKi1KFDB/Xo0UP16tVTUVGRStzd9l+OcLRu3TplZGT8UZmICGVkZCg7O7vc57z//vvq2bOnxowZo6SkJHXs2FGPP/647HZ7he9z7NgxlZSUuHxZwmOPeXbqd//+vqsLAABBxONwc91112nevHlnXF+wYIGuu+46t19n3759stvtSkpKcrmelJSkwsLCcp/zyy+/aNGiRbLb7Vq2bJkeeOABPfPMM3r00UcrfJ/p06crPj7e+ZWamup2HYOWpxv2paZKvXv7rj4AAAQRj8PNmjVrdOmll55xvU+fPlqzZo1XKlWR0tJSNWnSRC+99JK6deumIUOG6L777tOcOXMqfM6UKVNUXFzs/CooKPBpHf3Ckw37JGnmTCYSAwDChscTio8dO6aTJ0+ecf3EiRP6/fff3X6dxMRERUZGavfu3S7Xd+/ereTk5HKfk5KSotq1ayvylD/U55xzjgoLC3X8+HFFRUWd8Zzo6GhFR0e7Xa+g58mGfRER0vz5nPoNAAgrHvfc9OjRQy+99NIZ1+fMmaNu3bq5/TpRUVHq1q2bVqxY4bxWWlqqFStWqGfPnuU+p1evXsrLy1Npaanz2s8//6yUlJRyg40lebJhH6d+AwDCkMc9N48++qgyMjK0ceNG9e3bV5K0YsUKff311/r44489eq3x48drxIgR6t69u3r06KGZM2fq8OHDGjlypCRp+PDhatasmaZPny5Juv322/Wvf/1L48aN09ixY7V582Y9/vjjuvPOOz1tRuhautS9cvXqceo3ACAseRxuevXqpezsbD311FNasGCBYmNj1blzZ73yyitq06aNR681ZMgQ7d27Vw8++KAKCwvVtWtXLV++3DnJOD8/XxERf3Qupaam6qOPPtLdd9+tzp07q1mzZho3bpz+8Y9/eNqM0GS3S//zP+6VZcM+AECYqvY+N6EqpPe5ycqSypnMfYa4OMfQFeEGAGARnvz99rjnRnLMjcnLy9OePXtc5r9IKvdQTXiJu0NSN91EsAEAhC2Pw81XX32l66+/Xr/++qtO7/Sx2WyVbqiHGrDbpVdfda8sG/YBAMKYx+HmtttuU/fu3fXBBx8oJSVFNpvNF/XC6R57THJnd+XGjdmwDwAQ1jwON5s3b9aiRYvUunVrX9QH5fFkb5thwxiSAgCENY/3uUlPT1deXp4v6oKKeLK3DUNSAIAw53HPzdixYzVhwgQVFhaqU6dOZ5zO3blzZ69VDv/h7kTiRo0YkgIAhD2Pl4Kfuu+M80VsNhljQmJCccgtBbfbpeRkad++qstOmyY9+KDv6wQAgJ/5dCn41q1bq10xVMOqVe4Fm7g46b77fF8fAACCnMfhpkWLFr6oByqya5d75djbBgAASdXcxE+SfvzxR+Xn5+v48eMu16+55poaVwqn2LzZvXJMJAYAQFI1ws0vv/yiv/zlL/ruu++cc20kOfe7CfY5NyHFbpfKOYH9DM2bM5EYAID/8Hgp+Lhx49SyZUvt2bNHderU0Q8//KAvvvhC3bt3V1ZWlg+qGMYee0zasaPqcqNGMSQFAMB/eNxzk52drc8++0yJiYmKiIhQRESELrroIk2fPl133nmnNmzY4It6hp/MTGnqVPfKengaOwAAVuZxz43dblf9+vUlSYmJidq5c6ckx0Tj3Nxc79YuXNnt0rhx7pdPSfFdXQAACDEe99x07NhRGzduVMuWLZWenq5//vOfioqK0ksvvaRWrVr5oo7hZ9Uqaft298qmpjLfBgCAU3gcbu6//34dPnxYkvTwww/rqquuUu/evdWoUSPNnz/f6xUMS+7uSCxJM2cy3wYAgFN4vENxeYqKitSgQYOQOCE86HcoZkdiAADO4NMdisvTsGFDb7wMJPd3JE5MZEdiAADK4Va4GThwoF5//XXFxcVp4MCBlZbNzMz0SsXClrs7Ev/tbwxHAQBQDrfCTXx8vHPIKT4+3qcVCnvsSAwAQI14NOfGGKOCggI1btxYsbGxvqyXzwT1nBu7XWrRouqN+5o3l7Zto+cGABA2PPn77dE+N8YYtW7dWtvdXaYMz6xaxY7EAADUkEfhJiIiQm3atNH+/ft9VZ/w5u4ScHYkBgCgQh7vUPzEE09o0qRJ+v77731Rn/Blt0v/8z/ulWVHYgAAKuTxUvDhw4fryJEj6tKli6Kios6Ye1NUVOS1yoUVd5eAN27MjsQAAFTC43Azc+ZMH1QDbi8BHzaM+TYAAFTC43AzYsQIX9QDLAEHAMArarRD8dGjR3X8+HGXa0G3vDoU2O3SSy9VXa55c4akAACogscTig8fPqw77rhDTZo0Ud26ddWgQQOXL1QDS8ABAPAaj8PNPffco88++0wvvPCCoqOj9fLLL2vatGlq2rSp3nzzTV/U0frcnW/DEnAAAKrk8bDUv//9b7355pvq06ePRo4cqd69e6t169Zq0aKF3n77bQ0bNswX9bQ2d5d2swQcAIAqedxzU1RUpFatWklyzK8pW/p90UUX6YsvvvBu7cLF3r1Vl0lNZb4NAABu8DjctGrVSlu3bpUktW/fXgsWLJDk6NFJSEjwauXCgt0ujR9fdbkZM5hvAwCAGzwONyNHjtTGjRslSZMnT9bs2bMVExOju+++W5MmTfJ6BS1v1SrJnbO6EhN9XxcAACzA4zk3d999t/PnjIwM/fTTT1q3bp1at26tzp07e7VyYcHdycTulgMAIMx5HG4KCgqUmprqfNyiRQu1aNHCq5UKK+5u3sdkYgAA3OLxsFRaWpouueQSzZ07V7/99psv6hQ+2LwPAACv8zjcfPPNN+rRo4cefvhhpaSkaMCAAVq0aJGOHTvmi/pZG5v3AQDgdR6Hm3PPPVdPPfWU8vPz9eGHH6px48a65ZZblJSUpJtuuskXdbQuNu8DAMDrPA43ZWw2my699FLNnTtXn376qVq2bKk33njDm3WzPjbvAwDA66odbrZv365//vOf6tq1q3r06KF69epp9uzZ3qyb9bF5HwAAXufxaqkXX3xR77zzjlavXq327dtr2LBhWrp0KSumPMXmfQAA+ITH4ebRRx/V0KFD9eyzz6pLly6+qFN4YPM+AAB8wuNwk5+fL5vN5ou6hBc27wMAwCc8nnNDsPESJhMDAOAT1Z5QjBrau7fyuTQ2G5OJAQCoBo+HpeAFmZnSkCGSMZWXmzmTycQAAHiInht/s9ulceMqDzaRkdKCBdLAgf6rFwAAFlGjnpt9+/ZpzZo1stvtOv/885XC/JCqubNKym5nlRQAANVU7XCzePFi3XzzzWrbtq1OnDih3NxczZ49WyNHjvRm/ayHVVIAAPiU28NShw4dcnk8bdo0rV27VmvXrtWGDRu0cOFC3XfffV6voOWwSgoAAJ9yO9x069ZNS5cudT6uVauW9uzZ43y8e/duRUVFebd2VsSRCwAA+JTNmKqW7Dhs27ZNY8aMUVRUlGbPnq0tW7bouuuuk91u18mTJxUREaHXX39dV155pa/rXCMlJSWKj49XcXGx4uLi/PvmdruUllb1nJuFC6VBg/xSJQAAQoEnf7/dnnOTlpamDz74QO+++64uueQS3XnnncrLy1NeXp7sdrvat2+vmJiYGlfe0jhyAQAAn/N4KfjQoUP19ddfa+PGjerTp49KS0vVtWtXgo07mEwMAIDPebRaatmyZdq0aZO6dOmil19+WZ9//rmGDRumK664Qg8//LBiY2N9VU9rYDIxAAA+53bPzYQJEzRy5Eh9/fXXuvXWW/XII4/okksu0fr16xUTE6Nzzz1XH374oS/rGvp695aaN3ccrVAejlwAAKDG3J5Q3KhRI3388cfq1q2bioqKdMEFF+jnn3923v/xxx916623atWqVT6rrDcEdEKxJN1zj/TUU+Xfs9mkRYvYmRgAgNN48vfb7Z6bunXrauvWrZKkgoKCM+bYdOjQIeiDTcBlZkpPP13x/YkTCTYAANSQ2+Fm+vTpGj58uJo2bapLLrlEjzzyiC/rZT3unCk1b56jHAAAqDa3h6Ukaf/+/frll1/Upk0bJSQk+LBavhOwYamsLOnSS6sut3Kl1KePr2sDAEBI8ck+N5Jj3k2jRo1qVLmwxTJwAAD8wuN9blBNLAMHAMAvCDf+wjJwAAD8gnDjL5GR0tChlU8onjnTUQ4AAFQb4cZfWAYOAIBfEG78gWXgAAD4DeHGH9w5DbygwFEOAADUCOHGH1gGDgCA3xBu/IFl4AAA+E1QhJvZs2crLS1NMTExSk9P19q1a9163rx582Sz2TRgwADfVrCmWAYOAIDfBDzczJ8/X+PHj9fUqVO1fv16denSRf369dOePXsqfd62bds0ceJE9Q6FQBAZKc2aVf69ssDDMnAAALwi4OFmxowZGjVqlEaOHKkOHTpozpw5qlOnjl599dUKn2O32zVs2DBNmzZNrVq1qvT1jx07ppKSEpevgGnYsPxrixaxDBwAAC8JaLg5fvy41q1bp4yMDOe1iIgIZWRkKDs7u8LnPfzww2rSpIluvvnmKt9j+vTpio+Pd36lpqZ6pe4eycyUBg2S9u8/81551wAAQLUFNNzs27dPdrtdSUlJLteTkpJUWFhY7nO+/PJLvfLKK5o7d65b7zFlyhQVFxc7vwoKCmpcb49UtceNzSbddRd73AAA4CUenQoeaAcPHtQNN9yguXPnKjEx0a3nREdHKzo62sc1q0RVe9wY88ceN336+K1aAABYVUDDTWJioiIjI7V7926X67t371ZycvIZ5bds2aJt27bp6quvdl4rLS2VJNWqVUu5ubk6++yzfVtpT7HHDQAAfhXQYamoqCh169ZNK1ascF4rLS3VihUr1LNnzzPKt2/fXt99951ycnKcX9dcc40uvfRS5eTkBGY+TVXY4wYAAL8K+LDU+PHjNWLECHXv3l09evTQzJkzdfjwYY0cOVKSNHz4cDVr1kzTp09XTEyMOnbs6PL8hIQESTrjetAo2+Nmx47y593YbI77obCkHQCAEBDwcDNkyBDt3btXDz74oAoLC9W1a1ctX77cOck4Pz9fEREBX7FefWV73AwadOY99rgBAMDrbMZUdlS19ZSUlCg+Pl7FxcWKi4vzz5tmZkq33HLmsu9GjaSXXmKPGwAAquDJ3++A99xYXtkeN+VlSPa4AQDA60J4vCcEsMcNAAB+R7jxJU/2uAEAAF5BuPEl9rgBAMDvCDe+xB43AAD4HeHGl8r2uClb8n06m01KTWWPGwAAvIhw40tle9yUhz1uAADwCcKNrw0cKC1aJNU6bdV98+aO6+xxAwCAV7HPjT9cc80fPTWjRzsCTZ8+9NgAAOAD9Nz4Wmam1KKFdOKE4/Hzz0s33igtXRrQagEAYFWEG18q2514507X6zt2OK5nZgamXgAAWBjhxlcq25247Bq7EwMA4HWEG19hd2IAAAKCcOMr7E4MAEBAEG58hd2JAQAICMKNr7A7MQAAAUG48RV2JwYAICAIN75Utjvx6b037E4MAIDPsEOxL9ntUmzsH0u/586VWrd2DEXRYwMAgE/Qc+MrmZlSWpp05ZV/XJs2TSoqItgAAOBDhBtfKNuZ+PR9btiZGAAAnyPceBs7EwMAEFCEG29jZ2IAAAKKcONt7EwMAEBAEW68jZ2JAQAIKMKNt7EzMQAAAUW48bZTdyY+PeCwMzEAAD5HuPGFsp2JmzVzvc7OxAAA+Bw7FPvKwIHSpZdKDRs6Hi9bJl12GT02AAD4GOHGl3bvdnyPj5euuCKwdQEAIEwwLOUrdrujt0ZyhBs27QMAwC8IN75Qdq7UhAmOx/n5jsccuwAAgM8RbryNc6UAAAgowo03ca4UAAABR7jxJs6VAgAg4Ag33sS5UgAABBzhxps4VwoAgIAj3HgT50oBABBwhBtv4lwpAAACjnDjbWXnSiUnu17nXCkAAPyC4xd8YeBAR5hJT5cSEqT33nMMRdFjAwCAzxFufGXfPsf3li2lPn0CWhUAAMIJw1K+UnZoZlJSYOsBAECYIdz4gt0uZWc7fjaGHYkBAPAjwo23lR2aOXeu4/FHH3FoJgAAfkS48SYOzQQAIOAIN97CoZkAAAQFwo23cGgmAABBgXDjLRyaCQBAUCDceAuHZgIAEBQIN97CoZkAAAQFwo23cGgmAABBgXDjTWWHZjZu7HqdQzMBAPAbzpbytoEDpTp1pCuucISat97i0EwAAPyIcOMLBw44vp99NodmAgDgZwxL+cL+/Y7viYmBrQcAAGGIcOMLZeGmUaPA1gMAgDBEuPEFwg0AAAFDuPE2u1368UfHz7/9xllSAAD4GeHGmzIzpbQ06dNPHY/nzHE85jRwAAD8hnDjLZmZ0qBBZx6euWOH4zoBBwAAvyDceIPdLo0b5zj5+3Rl1+66iyEqAAD8gHDjDatWndljcypjpIICRzkAAOBThBtv2LXLu+UAAEC1EW68ISXFu+UAAEC1EW68oXdvxzlSp58GXsZmk1JTHeUAAIBPEW68ITJSmjWr/HtlgWfmTA7PBADADwg33jJwoLRokdS4sev15s0d1wcODEy9AAAIM5wK7k0DBzqWew8eLLVtK734omMoih4bAAD8Jih6bmbPnq20tDTFxMQoPT1da9eurbDs3Llz1bt3bzVo0EANGjRQRkZGpeX9rqTE8b1NG6lPH4INAAB+FvBwM3/+fI0fP15Tp07V+vXr1aVLF/Xr10979uwpt3xWVpaGDh2qlStXKjs7W6mpqbrsssu0Y8cOP9e8AgcOOL43aBDQagAAEK4CHm5mzJihUaNGaeTIkerQoYPmzJmjOnXq6NVXXy23/Ntvv63Ro0era9euat++vV5++WWVlpZqxYoV5ZY/duyYSkpKXL58qizcJCT49n0AAEC5Ahpujh8/rnXr1ikjI8N5LSIiQhkZGcrOznbrNY4cOaITJ06oYcOG5d6fPn264uPjnV+pqaleqXuFfvvN8Z1wAwBAQAQ03Ozbt092u11JSUku15OSklRYWOjWa/zjH/9Q06ZNXQLSqaZMmaLi4mLnV0FBQY3rXSmGpQAACKiQXi31xBNPaN68ecrKylJMTEy5ZaKjoxUdHe2/SjEsBQBAQAU03CQmJioyMlK7d+92ub57924lJydX+tynn35aTzzxhD799FN17tzZl9X0DOEGAICACuiwVFRUlLp16+YyGbhscnDPnj0rfN4///lPPfLII1q+fLm6d+/uj6q6x27/43TwbdscjwEAgF8FfLXU+PHjNXfuXL3xxhvatGmTbr/9dh0+fFgjR46UJA0fPlxTpkxxln/yySf1wAMP6NVXX1VaWpoKCwtVWFioQ4cOBaoJDpmZUlqa9OuvjscTJjgeZ2YGslYAAISdgM+5GTJkiPbu3asHH3xQhYWF6tq1q5YvX+6cZJyfn6+IiD8y2AsvvKDjx49r0KBBLq8zdepUPfTQQ/6s+h8yM6VBgyRjXK/v2OG4zvELAAD4jc2Y0/8iW1tJSYni4+NVXFysuLi4mr+g3e7ooSkbjjqdzeY4X2rrVnYrBgCgmjz5+x3wYamQt2pVxcFGcvTmFBQ4ygEAAJ8j3NTUrl3eLQcAAGqEcFNTKSneLQcAAGqEcFNTvXs75tTYbOXft9mk1FRHOQAA4HOEm5qKjJRmzSr/XlngmTmTycQAAPgJ4cYbBg50LPdu1Mj1evPmLAMHAMDPAr7PjWUMHCgdPSoNGyZ16CDNnu0YiqLHBgAAvyLceNPvvzu+t2ol9ekT0KoAABCuGJbypsOHHd/r1g1sPQAACGOEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28i3AAAEHCEG28xhnADAEAQINx4y5EjUmmp4+f16yW7PbD1AQAgTBFuvCEzU2rb9o/HV10lpaU5rgMAAL8i3NRUZqY0aJC0c6fr9R07HNcJOAAA+BXhpibsdmncOMd8m9OVXbvrLoaoAADwI8JNTaxaJW3fXvF9Y6SCAkc5AADgF4Sbmti1y7vlAABAjRFuaiIlxbvlAABAjRFuaqJ3b6l5c8lmK/++zSalpjrKAQAAvyDc1ERkpDRrluPn0wNO2eOZMx3lAACAXxBuamrgQGnRIqlZM9frzZs7rg8cGJh6AQAQpmoFugKWMHCg1L+/Y1XUrl2OOTa9e9NjAwBAABBuvCUyUurTJ9C1AAAg7DEsBQAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALIVwAwAALCXsdig2xkiSSkpKAlwTAADgrrK/22V/xysTduHm4MGDkqTU1NQA1wQAAHjq4MGDio+Pr7SMzbgTgSyktLRUO3fuVP369WWz2bz2uiUlJUpNTVVBQYHi4uK89rrBwurtk6zfRqu3T7J+G63ePsn6bbR6+yTftdEYo4MHD6pp06aKiKh8Vk3Y9dxERESoefPmPnv9uLg4y/7CStZvn2T9Nlq9fZL122j19knWb6PV2yf5po1V9diUYUIxAACwFMINAACwFMKNl0RHR2vq1KmKjo4OdFV8wurtk6zfRqu3T7J+G63ePsn6bbR6+6TgaGPYTSgGAADWRs8NAACwFMINAACwFMINAACwFMINAACwFMKNF8yePVtpaWmKiYlRenq61q5dG+gqVdtDDz0km83m8tW+fXvn/aNHj2rMmDFq1KiR6tWrp7/+9a/avXt3AGtcuS+++EJXX321mjZtKpvNpiVLlrjcN8bowQcfVEpKimJjY5WRkaHNmze7lCkqKtKwYcMUFxenhIQE3XzzzTp06JAfW1G5qtp44403nvGZXn755S5lgrmN06dP1/nnn6/69eurSZMmGjBggHJzc13KuPN7mZ+frz//+c+qU6eOmjRpokmTJunkyZP+bEq53Glfnz59zvgMb7vtNpcywdo+SXrhhRfUuXNn56ZuPXv21Icffui8H8qfn1R1+0L98zvdE088IZvNprvuust5Leg+Q4MamTdvnomKijKvvvqq+eGHH8yoUaNMQkKC2b17d6CrVi1Tp041//Vf/2V27drl/Nq7d6/z/m233WZSU1PNihUrzDfffGMuuOACc+GFFwawxpVbtmyZue+++0xmZqaRZN577z2X+0888YSJj483S5YsMRs3bjTXXHONadmypfn999+dZS6//HLTpUsX89VXX5lVq1aZ1q1bm6FDh/q5JRWrqo0jRowwl19+uctnWlRU5FImmNvYr18/89prr5nvv//e5OTkmCuvvNKcddZZ5tChQ84yVf1enjx50nTs2NFkZGSYDRs2mGXLlpnExEQzZcqUQDTJhTvtu+SSS8yoUaNcPsPi4mLn/WBunzHGvP/+++aDDz4wP//8s8nNzTX33nuvqV27tvn++++NMaH9+RlTdftC/fM71dq1a01aWprp3LmzGTdunPN6sH2GhJsa6tGjhxkzZozzsd1uN02bNjXTp08PYK2qb+rUqaZLly7l3jtw4ICpXbu2WbhwofPapk2bjCSTnZ3tpxpW3+l/+EtLS01ycrJ56qmnnNcOHDhgoqOjzbvvvmuMMebHH380kszXX3/tLPPhhx8am81mduzY4be6u6uicNO/f/8KnxNqbdyzZ4+RZD7//HNjjHu/l8uWLTMRERGmsLDQWeaFF14wcXFx5tixY/5tQBVOb58xjj+Op/4hOV0ota9MgwYNzMsvv2y5z69MWfuMsc7nd/DgQdOmTRvzySefuLQpGD9DhqVq4Pjx41q3bp0yMjKc1yIiIpSRkaHs7OwA1qxmNm/erKZNm6pVq1YaNmyY8vPzJUnr1q3TiRMnXNrbvn17nXXWWSHZ3q1bt6qwsNClPfHx8UpPT3e2Jzs7WwkJCerevbuzTEZGhiIiIrRmzRq/17m6srKy1KRJE7Vr106333679u/f77wXam0sLi6WJDVs2FCSe7+X2dnZ6tSpk5KSkpxl+vXrp5KSEv3www9+rH3VTm9fmbfffluJiYnq2LGjpkyZoiNHjjjvhVL77Ha75s2bp8OHD6tnz56W+/xOb18ZK3x+Y8aM0Z///GeXz0oKzv8Gw+7gTG/at2+f7Ha7y4clSUlJSfrpp58CVKuaSU9P1+uvv6527dpp165dmjZtmnr37q3vv/9ehYWFioqKUkJCgstzkpKSVFhYGJgK10BZncv7/MruFRYWqkmTJi73a9WqpYYNG4ZMmy+//HINHDhQLVu21JYtW3TvvffqiiuuUHZ2tiIjI0OqjaWlpbrrrrvUq1cvdezYUZLc+r0sLCws93MuuxcsymufJF1//fVq0aKFmjZtqm+//Vb/+Mc/lJubq8zMTEmh0b7vvvtOPXv21NGjR1WvXj2999576tChg3Jycizx+VXUPskan9+8efO0fv16ff3112fcC8b/Bgk3cHHFFVc4f+7cubPS09PVokULLViwQLGxsQGsGarruuuuc/7cqVMnde7cWWeffbaysrLUt2/fANbMc2PGjNH333+vL7/8MtBV8YmK2nfLLbc4f+7UqZNSUlLUt29fbdmyRWeffba/q1kt7dq1U05OjoqLi7Vo0SKNGDFCn3/+eaCr5TUVta9Dhw4h//kVFBRo3Lhx+uSTTxQTExPo6riFYakaSExMVGRk5Bkzwnfv3q3k5OQA1cq7EhIS1LZtW+Xl5Sk5OVnHjx/XgQMHXMqEanvL6lzZ55ecnKw9e/a43D958qSKiopCss2S1KpVKyUmJiovL09S6LTxjjvu0P/+7/9q5cqVat68ufO6O7+XycnJ5X7OZfeCQUXtK096erokuXyGwd6+qKgotW7dWt26ddP06dPVpUsXzZo1yzKfX0XtK0+ofX7r1q3Tnj17dN5556lWrVqqVauWPv/8cz377LOqVauWkpKSgu4zJNzUQFRUlLp166YVK1Y4r5WWlmrFihUuY62h7NChQ9qyZYtSUlLUrVs31a5d26W9ubm5ys/PD8n2tmzZUsnJyS7tKSkp0Zo1a5zt6dmzpw4cOKB169Y5y3z22WcqLS11/g8q1Gzfvl379+9XSkqKpOBvozFGd9xxh9577z199tlnatmypct9d34ve/bsqe+++84lxH3yySeKi4tzDh0ESlXtK09OTo4kuXyGwdq+ipSWlurYsWMh//lVpKx95Qm1z69v37767rvvlJOT4/zq3r27hg0b5vw56D5Dr09RDjPz5s0z0dHR5vXXXzc//vijueWWW0xCQoLLjPBQMmHCBJOVlWW2bt1qVq9ebTIyMkxiYqLZs2ePMcax3O+ss84yn332mfnmm29Mz549Tc+ePQNc64odPHjQbNiwwWzYsMFIMjNmzDAbNmwwv/76qzHGsRQ8ISHBLF261Hz77bemf//+5S4FP/fcc82aNWvMl19+adq0aRM0y6SNqbyNBw8eNBMnTjTZ2dlm69at5tNPPzXnnXeeadOmjTl69KjzNYK5jbfffruJj483WVlZLktpjxw54ixT1e9l2TLUyy67zOTk5Jjly5ebxo0bB8VS26ral5eXZx5++GHzzTffmK1bt5qlS5eaVq1amYsvvtj5GsHcPmOMmTx5svn888/N1q1bzbfffmsmT55sbDab+fjjj40xof35GVN5+6zw+ZXn9BVgwfYZEm684LnnnjNnnXWWiYqKMj169DBfffVVoKtUbUOGDDEpKSkmKirKNGvWzAwZMsTk5eU57//+++9m9OjRpkGDBqZOnTrmL3/5i9m1a1cAa1y5lStXGklnfI0YMcIY41gO/sADD5ikpCQTHR1t+vbta3Jzc11eY//+/Wbo0KGmXr16Ji4uzowcOdIcPHgwAK0pX2VtPHLkiLnssstM48aNTe3atU2LFi3MqFGjzgjfwdzG8tomybz22mvOMu78Xm7bts1cccUVJjY21iQmJpoJEyaYEydO+Lk1Z6qqffn5+ebiiy82DRs2NNHR0aZ169Zm0qRJLvukGBO87TPGmJtuusm0aNHCREVFmcaNG5u+ffs6g40xof35GVN5+6zw+ZXn9HATbJ+hzRhjvN8fBAAAEBjMuQEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAEAAJZCuAFwhtdff10JCQmBrgYAVAvhBggjN954o2w2m5544gmX60uWLJHNZgtQrcLDjTfeqAEDBgS6GkBYINwAYSYmJkZPPvmkfvvtt0BXBQB8gnADhJmMjAwlJydr+vTpVZZdsmSJ2rRpo5iYGPXr108FBQWVlt++fbuGDh2qhg0bqm7duurevbvWrFnjvP/CCy/o7LPPVlRUlNq1a6e33nrL5fk2m00vvviirrrqKtWpU0fnnHOOsrOzlZeXpz59+qhu3bq68MILtWXLFudzHnroIXXt2lUvvviiUlNTVadOHQ0ePFjFxcXOMqWlpXr44YfVvHlzRUdHq2vXrlq+fLnz/vHjx3XHHXcoJSVFMTExatGihcu/z4wZM9SpUyfVrVtXqampGj16tA4dOuS8XzaM99FHH+mcc85RvXr1dPnll2vXrl3OOr7xxhtaunSpbDabbDabsrKyJEkFBQUaPHiwEhIS1LBhQ/Xv31/btm1zvnZWVpZ69OihunXrKiEhQb169dKvv/5a5WcHhDPCDRBmIiMj9fjjj+u5557T9u3bKyx35MgRPfbYY3rzzTe1evVqHThwQNddd12F5Q8dOqRLLrlEO3bs0Pvvv6+NGzfqnnvuUWlpqSTpvffe07hx4zRhwgR9//33uvXWWzVy5EitXLnS5XUeeeQRDR8+XDk5OWrfvr2uv/563XrrrZoyZYq++eYbGWN0xx13uDwnLy9PCxYs0L///W8tX75cGzZs0OjRo533Z82apWeeeUZPP/20vv32W/Xr10/XXHONNm/eLEl69tln9f7772vBggXKzc3V22+/rbS0NOfzIyIi9Oyzz+qHH37QG2+8oc8++0z33HPPGf9eTz/9tN566y198cUXys/P18SJEyVJEydO1ODBg52BZ9euXbrwwgt14sQJ9evXT/Xr19eqVau0evVqZzA6fvy4Tp48qQEDBuiSSy7Rt99+q+zsbN1yyy0MIQJV8clZ4wCC0ogRI0z//v2NMcZccMEF5qabbjLGGPPee++ZU/938NprrxlJ5quvvnJe27Rpk5Fk1qxZU+5rv/jii6Z+/fpm//795d6/8MILzahRo1yuXXvttebKK690PpZk7r//fufj7OxsI8m88sorzmvvvvuuiYmJcT6eOnWqiYyMNNu3b3de+/DDD01ERITZtWuXMcaYpk2bmscee8zlvc8//3wzevRoY4wxY8eONf/93/9tSktLy6376RYuXGgaNWrkfFz275WXl+e8Nnv2bJOUlOR8fOq/fZm33nrLtGvXzuV9jx07ZmJjY81HH31k9u/fbySZrKwst+oFwIGeGyBMPfnkk3rjjTe0adOmcu/XqlVL559/vvNx+/btlZCQUGH5nJwcnXvuuWrYsGG59zdt2qRevXq5XOvVq9cZr9e5c2fnz0lJSZKkTp06uVw7evSoSkpKnNfOOussNWvWzPm4Z8+eKi0tVW5urkpKSrRz585K3/vGG29UTk6O2rVrpzvvvFMff/yxS9lPP/1Uffv2VbNmzVS/fn3dcMMN2r9/v44cOeIsU6dOHZ199tnOxykpKdqzZ0+5/xZlNm7cqLy8PNWvX1/16tVTvXr11LBhQx09elRbtmxRw4YNdeONN6pfv366+uqrNWvWLOdQF4CKEW6AMHXxxRerX79+mjJlildeLzY21iuvU7t2befPZcMv5V0rG+7yhvPOO09bt27VI488ot9//12DBw/WoEGDJEnbtm3TVVddpc6dO2vx4sVat26dZs+eLckxV6e8epfV0xhT6fseOnRI3bp1U05OjsvXzz//rOuvv16S9Nprryk7O1sXXnih5s+fr7Zt2+qrr77yWtsBKyLcAGHsiSee0L///W9lZ2efce/kyZP65ptvnI9zc3N14MABnXPOOeW+VufOnZWTk6OioqJy759zzjlavXq1y7XVq1erQ4cONWiBQ35+vnbu3Ol8/NVXXykiIkLt2rVTXFycmjZtWuV7x8XFaciQIZo7d67mz5+vxYsXq6ioSOvWrVNpaameeeYZXXDBBWrbtq3Le7krKipKdrvd5dp5552nzZs3q0mTJmrdurXLV3x8vLPcueeeqylTpuj//u//1LFjR73zzjsevz8QTgg3QBjr1KmThg0bpmefffaMe7Vr19bYsWO1Zs0arVu3TjfeeKMuuOAC9ejRo9zXGjp0qJKTkzVgwACtXr1av/zyixYvXuwMTpMmTdLrr7+uF154QZs3b9aMGTOUmZnpnHRbEzExMRoxYoQ2btyoVatW6c4779TgwYOVnJzsfO8nn3xS8+fPV25uriZPnqycnByNGzdOkmM11LvvvquffvpJP//8sxYuXKjk5GQlJCSodevWOnHihJ577jn98ssveuuttzRnzhyP65iWlqZvv/1Wubm52rdvn06cOKFhw4YpMTFR/fv316pVq7R161ZlZWXpzjvv1Pbt27V161ZNmTJF2dnZ+vXXX/Xxxx9r8+bNFQZMAP8R6Ek/APynvEmtW7duNVFRUWdMKI6PjzeLFy82rVq1MtHR0SYjI8P8+uuvlb7+tm3bzF//+lcTFxdn6tSpY7p37+4yAfn55583rVq1MrVr1zZt27Y1b775psvzJZn33nvPpW6SzIYNG5zXVq5caSSZ3377zRjjmFDcpUsX8/zzz5umTZuamJgYM2jQIFNUVOR8jt1uNw899JBp1qyZqV27tunSpYv58MMPnfdfeukl07VrV1O3bl0TFxdn+vbta9avX++8P2PGDJOSkmJiY2NNv379zJtvvulSh7J/r1OdPkl7z5495k9/+pOpV6+ekWRWrlxpjDFm165dZvjw4SYxMdFER0ebVq1amVGjRpni4mJTWFhoBgwYYFJSUkxUVJRp0aKFefDBB43dbq/0cwDCnc2YKgaFASCIPfTQQ1qyZIlycnICXRUAQYJhKQAAYCmEGwAAYCkMSwEAAEuh5wYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFgK4QYAAFjK/weJeTizNWPdIAAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "388 composantes principales expliquent au moins 95% de la variance totale\n",
            "Nombre de composantes principales expliquant 95% de la variance :  388\n"
          ]
        }
      ],
      "source": [
        "df_pca = preprocess_pca(features_df)\n",
        "k = recherche_nb_composante(df_pca)\n",
        "\n",
        "print(\"Nombre de composantes principales expliquant 95% de la variance : \", k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcDER-R13ojH",
        "outputId": "6919f1d2-d0e9-44fb-8503-dcfd4a6d205b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UsageError: Line magic function `%matplot` not found.\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplot plt\n",
        "n_components = recherche_nb_composante(df_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "xc10VfZ43onY"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.ml.feature import StandardScaler, PCA\n",
        "from pyspark.ml.linalg import Vectors, VectorUDT\n",
        "from pyspark.sql.functions import udf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "PdJa2ofNBZHB"
      },
      "outputs": [],
      "source": [
        "pca = PCA(k=k, inputCol=\"features_scaled\", outputCol=\"features_pca\")\n",
        "model_pca = pca.fit(df_pca)\n",
        "df_reduit = model_pca.transform(df_pca)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8EtCdJT51lr",
        "outputId": "b45395f9-f9ce-424f-fc9d-e4103388d26f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|                path|label|            features|    features_vectors|     features_scaled|        features_pca|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "|file:/content/dri...|     |[0.33079213, 0.03...|[0.33079212903976...|[-0.0784443825770...|[8.83797407972369...|\n",
            "|file:/content/dri...|     |[0.7513627, 1.620...|[0.75136268138885...|[0.90946716629711...|[12.2164258525162...|\n",
            "|file:/content/dri...|     |[0.016479073, 0.4...|[0.01647907309234...|[-0.8167592650575...|[6.13228871317821...|\n",
            "|file:/content/dri...|     |[0.067617506, 0.3...|[0.06761750578880...|[-0.6966361419098...|[3.33698603994719...|\n",
            "|file:/content/dri...|     |[0.0, 0.85379255,...|[0.0,0.8537925481...|[-0.8554682677011...|[18.7622927820455...|\n",
            "|file:/content/dri...|     |[0.34845024, 0.02...|[0.34845024347305...|[-0.0369658361535...|[-2.7473403220890...|\n",
            "|file:/content/dri...|     |[1.541634, 0.3627...|[1.5416339635849,...|[2.7657981007013,...|[7.81496512653500...|\n",
            "|file:/content/dri...|     |[0.50183904, 0.23...|[0.50183904170989...|[0.32334128568304...|[-4.7224545177373...|\n",
            "|file:/content/dri...|     |[0.0, 1.1315749, ...|[0.0,1.1315748691...|[-0.8554682677011...|[6.92087001445911...|\n",
            "|file:/content/dri...|     |[0.20974927, 0.33...|[0.20974926650524...|[-0.3627715689164...|[2.56623052794666...|\n",
            "|file:/content/dri...|     |[0.23668702, 0.19...|[0.23668701946735...|[-0.2994953430767...|[1.73939428461308...|\n",
            "|file:/content/dri...|     |[1.2100002, 0.003...|[1.21000015735626...|[1.98679713322883...|[-1.9466131307443...|\n",
            "|file:/content/dri...|     |[1.0812459, 0.081...|[1.08124589920043...|[1.68435603770181...|[1.88780767981482...|\n",
            "|file:/content/dri...|     |[0.89299774, 0.00...|[0.89299774169921...|[1.24216500028611...|[-3.0474629058983...|\n",
            "|file:/content/dri...|     |[0.88253194, 0.58...|[0.88253194093704...|[1.21758105026055...|[18.1954737017072...|\n",
            "|file:/content/dri...|     |[0.9348819, 0.796...|[0.93488192558288...|[1.34055008393671...|[15.1988323539238...|\n",
            "|file:/content/dri...|     |[0.16921404, 0.11...|[0.16921404004096...|[-0.4579879794250...|[-8.8430488488815...|\n",
            "|file:/content/dri...|     |[0.06056695, 0.0,...|[0.06056695058941...|[-0.7131977505829...|[-3.9599845133161...|\n",
            "|file:/content/dri...|     |[0.0029379593, 0....|[0.00293795927427...|[-0.8485670619914...|[-6.6437031589118...|\n",
            "|file:/content/dri...|     |[0.86853975, 0.0,...|[0.86853975057601...|[1.18471368383537...|[-8.9833025627182...|\n",
            "+--------------------+-----+--------------------+--------------------+--------------------+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df_reduit.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sauvegarde des résultats sur S3\n",
        "output_path = \"s3a://oc-calculddist/Projet_agritech/data/Results\"\n",
        "df_reduit.write.parquet(output_path)\n",
        "print(f\"Les résultats ont été sauvegardés sur {output_path}\")"
      ],
      "metadata": {
        "id": "6ffHtNyR31TM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bfrz7LaL51pf",
        "outputId": "b2c47cb4-f35d-4384-ebe9-705ee3f29148"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Results\n"
          ]
        }
      ],
      "source": [
        "print(PATH_Result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "0SeDkK3X51uj"
      },
      "outputs": [],
      "source": [
        "#sauvegarde des données apres pca\n",
        "df_reduit.write.mode(\"overwrite\").parquet(PATH_Result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "50CJ-OXbD_xx"
      },
      "outputs": [],
      "source": [
        "# Sauvegarde des données - sans pca\n",
        "features_df.write.mode(\"overwrite\").parquet(PATH_Result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "i3ItNEtLD_4n",
        "outputId": "07252794-8344-425a-a294-eba9e7cad606"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                path label  \\\n",
              "0  file:/content/drive/My Drive/Colab Notebooks/P...         \n",
              "1  file:/content/drive/My Drive/Colab Notebooks/P...         \n",
              "2  file:/content/drive/My Drive/Colab Notebooks/P...         \n",
              "3  file:/content/drive/My Drive/Colab Notebooks/P...         \n",
              "4  file:/content/drive/My Drive/Colab Notebooks/P...         \n",
              "\n",
              "                                            features  \n",
              "0  [0.33079213, 0.03833661, 1.8054144, 0.0, 0.333...  \n",
              "1  [0.7513627, 1.620253, 1.230905, 0.0, 1.6259501...  \n",
              "2  [0.016479073, 0.48049733, 1.9058404, 0.0, 0.09...  \n",
              "3  [0.067617506, 0.34331834, 0.4419498, 0.0, 0.11...  \n",
              "4  [0.0, 0.85379255, 2.163731, 0.0, 2.889047, 0.0...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-68535c90-1ac0-4eb9-a332-aab62f26d08a\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>path</th>\n",
              "      <th>label</th>\n",
              "      <th>features</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td></td>\n",
              "      <td>[0.33079213, 0.03833661, 1.8054144, 0.0, 0.333...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td></td>\n",
              "      <td>[0.7513627, 1.620253, 1.230905, 0.0, 1.6259501...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td></td>\n",
              "      <td>[0.016479073, 0.48049733, 1.9058404, 0.0, 0.09...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td></td>\n",
              "      <td>[0.067617506, 0.34331834, 0.4419498, 0.0, 0.11...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>file:/content/drive/My Drive/Colab Notebooks/P...</td>\n",
              "      <td></td>\n",
              "      <td>[0.0, 0.85379255, 2.163731, 0.0, 2.889047, 0.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-68535c90-1ac0-4eb9-a332-aab62f26d08a')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-68535c90-1ac0-4eb9-a332-aab62f26d08a button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-68535c90-1ac0-4eb9-a332-aab62f26d08a');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f43c679f-57cc-475d-9040-9c1da3fa285b\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f43c679f-57cc-475d-9040-9c1da3fa285b')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f43c679f-57cc-475d-9040-9c1da3fa285b button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 3110,\n  \"fields\": [\n    {\n      \"column\": \"path\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3110,\n        \"samples\": [\n          \"file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/apple_crimson_snow_1/r1_319.jpg\",\n          \"file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/zucchini_1/r0_67.jpg\",\n          \"file:/content/drive/My Drive/Colab Notebooks/Projet_agritech/data/Test/zucchini_dark_1/r0_35.jpg\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"features\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "df = pd.read_parquet(PATH_Result, engine='pyarrow')\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eyyhOSrHJIf"
      },
      "source": [
        "# 5. Conclusion\n",
        "Ce projet nous a permis d'acquérir des compétences en déploiement de solutions cloud avec Big Data.\n",
        "\n",
        "Initialement, nous avons développé notre solution localement sur une machine virtuelle sous Windows en utilisant Colab. Après avoir installé l'environnement Spark et validé son fonctionnement avec un jeu de données réduit, nous avons opté pour le transfert learning à partir du modèle MobileNetV2 pour sa légèreté et sa rapidité.\n",
        "\n",
        "Les résultats ont été enregistrés en format \"parquet\" et la solution a fonctionné parfaitement en mode local. Ensuite, nous avons créé un cluster de calcul sur Amazon Web Services (AWS) pour anticiper une augmentation de la charge de travail. Nous avons utilisé EC2 pour louer de la puissance de calcul et EMR pour instancier et configurer un cluster complet avec Spark, Hadoop et TensorFlow.\n",
        "\n",
        "Enfin, nous avons utilisé Amazon S3 pour stocker nos données, bénéficiant ainsi d'un espace de stockage potentiellement illimité avec des coûts proportionnels à l'utilisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTcXxD55JnJ_"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}